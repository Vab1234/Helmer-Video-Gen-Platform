{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25dd76a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: soundfile in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: open-clip-torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: future in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.23.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.28.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.5.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (1.0.20)\n",
      "Requirement already satisfied: filelock in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numba>=0.51.0->librosa) (0.45.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm>=1.0.17->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai requests python-dotenv Pillow numpy soundfile librosa ffmpeg-python tqdm open-clip-torch torch\n",
    "# System-level: ffmpeg must be installed on the machine (apt install ffmpeg / brew install ffmpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471220a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ace28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM plan: {'clean_prompt': 'a girl talking to a man', 'num_images': 5, 'num_videos': 0, 'num_audio': 0, 'notes': ''}\n",
      "Fetched total 5 assets from providers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|██████████| 5/5 [00:01<00:00,  4.70it/s]\n",
      "c:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:651: UserWarning: Not enough free disk space to download the file. The expected file size is: 689.88 MB. The target location C:\\Users\\JENNIFER\\.cache\\huggingface\\hub\\models--kakaobrain--align-base\\blobs only has 342.07 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 5 images with ALIGN...\n",
      "\n",
      "Top results:\n",
      "1. [image] pexels id=5710922 score=0.1274\n",
      "    title: A group therapy session with six adults seated in a circle, discussing support and mental health.\n",
      "    url  : https://images.pexels.com/photos/5710922/pexels-photo-5710922.jpeg\n",
      "\n",
      "2. [image] pexels id=5711017 score=0.1199\n",
      "    title: A diverse group of people sitting in a circle during a therapy session in a sports hall.\n",
      "    url  : https://images.pexels.com/photos/5711017/pexels-photo-5711017.jpeg\n",
      "\n",
      "3. [image] pexels id=6668312 score=0.1123\n",
      "    title: Father and daughter share a bonding moment reading a storybook indoors.\n",
      "    url  : https://images.pexels.com/photos/6668312/pexels-photo-6668312.jpeg\n",
      "\n",
      "4. [image] pexels id=5710988 score=0.1099\n",
      "    title: A group therapy session indoors with diverse adults in a supportive environment.\n",
      "    url  : https://images.pexels.com/photos/5710988/pexels-photo-5710988.jpeg\n",
      "\n",
      "5. [image] pexels id=5711382 score=0.0868\n",
      "    title: Black and white photo of a support group session indoors with diverse participants.\n",
      "    url  : https://images.pexels.com/photos/5711382/pexels-photo-5711382.jpeg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "llm_fetch_align_pipeline.py\n",
    "\n",
    "End-to-end script using Hugging Face ALIGN (kakaobrain/align-base):\n",
    "1) parse user prompt with OpenAI LLM to determine: cleaned prompt, #assets, which types (image/video/audio)\n",
    "2) fetch metadata from Pexels (images + videos) and Freesound (audio) accordingly\n",
    "3) download thumbnails/previews in-memory and preprocess (images or audio -> mel images)\n",
    "4) embed prompt and assets using ALIGN (AlignProcessor + AlignModel) and compute cosine similarity (relevance)\n",
    "5) print top-K per type\n",
    "\n",
    "Environment variables:\n",
    "  OPENAI_API_KEY, PEXELS_API_KEY, FREESOUND_API_KEY\n",
    "\n",
    "NOTE: This script does not save files to disk.\n",
    "\"\"\"\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# audio libs\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import ffmpeg\n",
    "\n",
    "# OpenAI (both old openai import and new OpenAI client usage retained from your original script)\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "# Transformers ALIGN\n",
    "import torch\n",
    "from transformers import AlignModel, AlignProcessor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------\n",
    "# Config & env\n",
    "# -------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PEXELS_KEY = os.getenv(\"PEXELS_API_KEY\")\n",
    "FREESOUND_KEY = os.getenv(\"FREESOUND_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in env\")\n",
    "if not PEXELS_KEY:\n",
    "    print(\"[WARN] No PEXELS_API_KEY found; Pexels fetching will return empty.\")\n",
    "if not FREESOUND_KEY:\n",
    "    print(\"[WARN] No FREESOUND_API_KEY found; Freesound fetching will return empty.\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "client = OpenAI()  # used for LLM parsing\n",
    "\n",
    "# -------------------------\n",
    "# 1) LLM prompt parser (OpenAI)\n",
    "# -------------------------\n",
    "def parse_prompt_with_openai(raw_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses the new openai client API:\n",
    "      client.chat.completions.create(...)\n",
    "    Returns a dict with clean_prompt, num_images, num_videos, num_audio, notes\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a helpful assistant that converts an unstructured user prompt \"\n",
    "        \"into a structured search plan. Return JSON only with fields: clean_prompt (string), \"\n",
    "        \"num_images (int), num_videos (int), num_audio (int), notes (string). \"\n",
    "        \"Rules:\\n\"\n",
    "        \"- If the user didn't request videos, set num_videos to 0.\\n\"\n",
    "        \"- If the user didn't request audio, set num_audio to 0.\\n\"\n",
    "        \"- Make counts sensible (1-10 per type). Be conservative if uncertain.\\n\"\n",
    "        \"- Clean the prompt for image/video/audio search (no extra commentary).\"\n",
    "    )\n",
    "    user = f\"User prompt: {raw_prompt}\\n\\nReturn JSON only.\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        text = resp.choices[0].message[\"content\"]\n",
    "    except Exception:\n",
    "        try:\n",
    "            text = resp.choices[0].message.content\n",
    "        except Exception:\n",
    "            text = str(resp)\n",
    "\n",
    "    try:\n",
    "        j = json.loads(text)\n",
    "        return {\n",
    "            \"clean_prompt\": j.get(\"clean_prompt\") or raw_prompt,\n",
    "            \"num_images\": int(j.get(\"num_images\") or 0),\n",
    "            \"num_videos\": int(j.get(\"num_videos\") or 0),\n",
    "            \"num_audio\": int(j.get(\"num_audio\") or 0),\n",
    "            \"notes\": j.get(\"notes\",\"\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"[warn] OpenAI response not parseable as JSON; falling back. Raw text:\", text[:400])\n",
    "        return {\n",
    "            \"clean_prompt\": raw_prompt,\n",
    "            \"num_images\": 5,\n",
    "            \"num_videos\": 0,\n",
    "            \"num_audio\": 0,\n",
    "            \"notes\": \"fallback heuristic used\"\n",
    "        }\n",
    "\n",
    "# -------------------------\n",
    "# 2) Fetchers (Pexels & Freesound)\n",
    "# -------------------------\n",
    "def unified_record(provider, id, typ, title, description, url, thumbnail_url=None,\n",
    "                   duration=None, uploader=None, published_at=None, tags=None, raw=None):\n",
    "    return {\n",
    "        \"provider\": provider,\n",
    "        \"id\": str(id),\n",
    "        \"type\": typ,  # image|video|audio\n",
    "        \"title\": title or \"\",\n",
    "        \"description\": description,\n",
    "        \"url\": url,\n",
    "        \"thumbnail_url\": thumbnail_url,\n",
    "        \"duration\": duration,\n",
    "        \"uploader\": uploader,\n",
    "        \"published_at\": published_at,\n",
    "        \"tags\": tags or [],\n",
    "        \"raw_meta\": raw or {}\n",
    "    }\n",
    "\n",
    "def search_pexels_images(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/v1/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"photos\", []):\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"image\",\n",
    "            title=it.get(\"alt\") or \"\",\n",
    "            description=None,\n",
    "            url=(it.get(\"src\") or {}).get(\"original\"),\n",
    "            thumbnail_url=(it.get(\"src\") or {}).get(\"medium\"),\n",
    "            duration=None,\n",
    "            uploader=it.get(\"photographer\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_pexels_videos(prompt: str, per_page: int = 8) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"videos\", []):\n",
    "        file_url = None\n",
    "        for vf in it.get(\"video_files\", []) or []:\n",
    "            if vf.get(\"quality\") == \"hd\":\n",
    "                file_url = vf.get(\"link\"); break\n",
    "        if not file_url and it.get(\"video_files\"):\n",
    "            file_url = it.get(\"video_files\")[0].get(\"link\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"video\",\n",
    "            title=(it.get(\"user\") or {}).get(\"name\") or str(it.get(\"id\")),\n",
    "            description=it.get(\"url\"),\n",
    "            url=file_url,\n",
    "            thumbnail_url=it.get(\"image\"),\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=(it.get(\"user\") or {}).get(\"name\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_freesound(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not FREESOUND_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://freesound.org/apiv2/search/text/\"\n",
    "    headers = {\"Authorization\": f\"Token {FREESOUND_KEY}\"}\n",
    "    params = {\"query\": prompt, \"page_size\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"results\", []):\n",
    "        preview = (it.get(\"previews\") or {}).get(\"preview-hq-mp3\") or (it.get(\"previews\") or {}).get(\"preview-lq-mp3\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"freesound\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"audio\",\n",
    "            title=it.get(\"name\"),\n",
    "            description=it.get(\"description\"),\n",
    "            url=preview,\n",
    "            thumbnail_url=None,\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=it.get(\"username\"),\n",
    "            published_at=it.get(\"created\"),\n",
    "            tags=it.get(\"tags\") or [],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def fetch_assets(clean_prompt: str, num_images:int, num_videos:int, num_audio:int) -> List[Dict]:\n",
    "    results = []\n",
    "    if num_images > 0:\n",
    "        imgs = search_pexels_images(clean_prompt, per_page=num_images)\n",
    "        results.extend(imgs[:num_images])\n",
    "    if num_videos > 0:\n",
    "        vids = search_pexels_videos(clean_prompt, per_page=num_videos)\n",
    "        results.extend(vids[:num_videos])\n",
    "    if num_audio > 0:\n",
    "        aud = search_freesound(clean_prompt, per_page=num_audio)\n",
    "        results.extend(aud[:num_audio])\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# 3) In-memory download & preprocessing to ALIGN-ready inputs\n",
    "# -------------------------\n",
    "def download_bytes(url: str, timeout=20) -> Optional[bytes]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, stream=True)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        print(\"[warn] download failed:\", e)\n",
    "        return None\n",
    "\n",
    "def download_image_pil(url: str, timeout=15) -> Optional[Image.Image]:\n",
    "    b = download_bytes(url, timeout=timeout)\n",
    "    if not b:\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"[warn] PIL open failed:\", e)\n",
    "        return None\n",
    "\n",
    "def center_crop_and_resize_pil(img: Image.Image, size:int=224) -> Image.Image:\n",
    "    w,h = img.size\n",
    "    m = min(w,h)\n",
    "    left = (w-m)//2\n",
    "    top = (h-m)//2\n",
    "    img = img.crop((left, top, left+m, top+m))\n",
    "    img = img.resize((size,size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def pil_to_chw_float(img: Image.Image, size:int=224, normalize:bool=True) -> np.ndarray:\n",
    "    img = center_crop_and_resize_pil(img, size=size)\n",
    "    arr = np.asarray(img).astype(np.float32)/255.0  # H W C\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()  # C H W\n",
    "    if normalize:\n",
    "        arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "# audio decode & mel -> 3-channel image-like tensor (C,H,W float in [-1,1])\n",
    "def decode_audio_bytes_to_waveform(audio_bytes: bytes, target_sr:int=16000, duration:float=5.0) -> Optional[np.ndarray]:\n",
    "    if not audio_bytes:\n",
    "        return None\n",
    "    try:\n",
    "        bio = io.BytesIO(audio_bytes)\n",
    "        data, sr = sf.read(bio, dtype='float32')\n",
    "        if data.ndim > 1:\n",
    "            data = np.mean(data, axis=1)\n",
    "        if sr != target_sr:\n",
    "            data = librosa.resample(data, sr, target_sr)\n",
    "        desired = int(target_sr * duration)\n",
    "        if len(data) > desired:\n",
    "            start = max(0, (len(data)-desired)//2)\n",
    "            data = data[start:start+desired]\n",
    "        elif len(data) < desired:\n",
    "            data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "        return data.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        # fallback: ffmpeg\n",
    "        try:\n",
    "            proc = (\n",
    "                ffmpeg.input('pipe:0')\n",
    "                .output('pipe:1', format='f32le', ar=target_sr, ac=1)\n",
    "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True)\n",
    "            )\n",
    "            out, err = proc.communicate(input=audio_bytes)\n",
    "            if proc.returncode != 0:\n",
    "                raise RuntimeError(\"ffmpeg decode fail\")\n",
    "            data = np.frombuffer(out, dtype=np.float32)\n",
    "            desired = int(target_sr * duration)\n",
    "            if len(data) > desired:\n",
    "                start = max(0, (len(data)-desired)//2)\n",
    "                data = data[start:start+desired]\n",
    "            elif len(data) < desired:\n",
    "                data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "            return data.astype(np.float32)\n",
    "        except Exception as e2:\n",
    "            print(\"[warn] audio decode failed:\", e, e2)\n",
    "            return None\n",
    "\n",
    "def waveform_to_mel_image_tensor(wav: np.ndarray, sr:int=16000, n_mels:int=128, n_fft:int=2048, hop_length:int=512, size:int=224) -> Optional[np.ndarray]:\n",
    "    if wav is None:\n",
    "        return None\n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-9)\n",
    "    img_arr = (S_norm * 255.0).astype(np.uint8)  # H(n_mels) x T\n",
    "    pil = Image.fromarray(img_arr)\n",
    "    pil = pil.resize((size,size), Image.LANCZOS).convert(\"RGB\")\n",
    "    arr = np.asarray(pil).astype(np.float32)/255.0\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()\n",
    "    arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "def preprocess_records(records: List[Dict], image_size:int=224, audio_duration:float=5.0, audio_sr:int=16000) -> List[Dict]:\n",
    "    processed = []\n",
    "    for r in tqdm(records, desc=\"preprocess\"):\n",
    "        rec = dict(r)\n",
    "        typ = rec[\"type\"]\n",
    "        if typ == \"image\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_chw_float(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"video\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_chw_float(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"audio\":\n",
    "            url = rec.get(\"url\")\n",
    "            audio_bytes = download_bytes(url)\n",
    "            wav = decode_audio_bytes_to_waveform(audio_bytes, target_sr=audio_sr, duration=audio_duration)\n",
    "            rec[\"waveform\"] = wav\n",
    "            rec[\"audio_mel_tensor\"] = waveform_to_mel_image_tensor(wav, sr=audio_sr, size=image_size) if wav is not None else None\n",
    "            # also keep a PIL for processor\n",
    "            if rec.get(\"audio_mel_tensor\") is not None:\n",
    "                # convert CHW float [-1,1] to PIL HWC uint8 via same conversion below when embedding\n",
    "                pass\n",
    "        else:\n",
    "            rec[\"img_tensor\"] = None\n",
    "            rec[\"audio_mel_tensor\"] = None\n",
    "        processed.append(rec)\n",
    "    return processed\n",
    "\n",
    "# -------------------------\n",
    "# 4) ALIGN embedding & scoring (Hugging Face AlignModel + AlignProcessor)\n",
    "# -------------------------\n",
    "class ALIGNEmbedder:\n",
    "    \"\"\"\n",
    "    Embedder wrapper around Hugging Face AlignModel + AlignProcessor.\n",
    "    model_name_or_path: e.g. \"kakaobrain/align-base\"\n",
    "    Works with inputs that are:\n",
    "      - numpy arrays shaped (3, H, W) float in [-1,1] or [0,1] or uint8 [0,255]\n",
    "      - numpy arrays shaped (H, W, 3) uint8 or float\n",
    "      - PIL.Image\n",
    "    Returns L2-normalized embeddings as numpy arrays.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name_or_path: str = \"kakaobrain/align-base\", device: Optional[str] = None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "\n",
    "        # load processor + model\n",
    "        try:\n",
    "            self.processor = AlignProcessor.from_pretrained(model_name_or_path)\n",
    "            self.model = AlignModel.from_pretrained(model_name_or_path).to(self.device).eval()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load ALIGN model/processor '{model_name_or_path}': {e}\")\n",
    "\n",
    "        try:\n",
    "            self.embedding_dim = self.model.config.projection_dim if hasattr(self.model.config, \"projection_dim\") else 640\n",
    "        except Exception:\n",
    "            self.embedding_dim = 640\n",
    "\n",
    "        try:\n",
    "            self.dtype = next(self.model.parameters()).dtype\n",
    "        except Exception:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "    def _convert_to_processor_image(self, arr_or_pil: Union[np.ndarray, Image.Image]):\n",
    "        if isinstance(arr_or_pil, Image.Image):\n",
    "            return arr_or_pil\n",
    "        if not isinstance(arr_or_pil, np.ndarray):\n",
    "            return arr_or_pil\n",
    "\n",
    "        arr = arr_or_pil\n",
    "        # If CHW convert to HWC\n",
    "        if arr.ndim == 3 and arr.shape[0] == 3:  # C, H, W\n",
    "            arr = np.transpose(arr, (1,2,0))\n",
    "        # If floats in [-1,1], convert to [0,255]\n",
    "        if np.issubdtype(arr.dtype, np.floating):\n",
    "            if arr.max() <= 1.0 + 1e-6 and arr.min() >= -1.0 - 1e-6:\n",
    "                arr = ((arr + 1.0) / 2.0 * 255.0).clip(0,255).astype(np.uint8)\n",
    "            elif arr.max() <= 1.0 + 1e-6 and arr.min() >= 0.0 - 1e-6:\n",
    "                arr = (arr * 255.0).clip(0,255).astype(np.uint8)\n",
    "            else:\n",
    "                arr = arr.clip(0,255).astype(np.uint8)\n",
    "        elif np.issubdtype(arr.dtype, np.integer):\n",
    "            arr = arr.astype(np.uint8)\n",
    "        else:\n",
    "            arr = arr.astype(np.uint8)\n",
    "        return Image.fromarray(arr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_images(self, imgs: List[Union[np.ndarray, Image.Image]], batch_size: int = 16) -> np.ndarray:\n",
    "        if len(imgs) == 0:\n",
    "            return np.zeros((0, self.embedding_dim), dtype=np.float32)\n",
    "\n",
    "        embs = []\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            proc_imgs = [self._convert_to_processor_image(x) for x in batch]\n",
    "            proc_out = self.processor(images=proc_imgs, return_tensors=\"pt\")\n",
    "            pixel_values = proc_out[\"pixel_values\"].to(self.device).type(self.dtype)\n",
    "            img_feats = self.model.get_image_features(pixel_values=pixel_values)\n",
    "            img_feats = img_feats / (img_feats.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "            embs.append(img_feats.cpu().numpy())\n",
    "        return np.vstack(embs).astype(np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        if len(texts) == 0:\n",
    "            return np.zeros((0, self.embedding_dim), dtype=np.float32)\n",
    "\n",
    "        embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            proc_out = self.processor(text=chunk, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = proc_out[\"input_ids\"].to(self.device)\n",
    "            attention_mask = proc_out.get(\"attention_mask\")\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "                text_feats = self.model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                text_feats = self.model.get_text_features(input_ids=input_ids)\n",
    "            text_feats = text_feats / (text_feats.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "            embs.append(text_feats.cpu().numpy())\n",
    "        return np.vstack(embs).astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_sim_matrix(img_embs: np.ndarray, txt_embs: np.ndarray) -> np.ndarray:\n",
    "        if img_embs.size == 0 or txt_embs.size == 0:\n",
    "            return np.zeros((img_embs.shape[0], txt_embs.shape[0]), dtype=np.float32)\n",
    "        a = img_embs / (np.linalg.norm(img_embs, axis=1, keepdims=True) + 1e-12)\n",
    "        b = txt_embs / (np.linalg.norm(txt_embs, axis=1, keepdims=True) + 1e-12)\n",
    "        return a.dot(b.T)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Pipeline orchestration (uses ALIGNEmbedder)\n",
    "# -------------------------\n",
    "def pipeline_run(user_prompt: str, top_k:int=5, align_model_id: str = \"kakaobrain/align-base\"):\n",
    "    # 1) parse prompt via OpenAI LLM\n",
    "    plan = parse_prompt_with_openai(user_prompt)\n",
    "    print(\"LLM plan:\", plan)\n",
    "\n",
    "    # 2) fetch assets according to plan\n",
    "    recs = fetch_assets(plan[\"clean_prompt\"], plan[\"num_images\"], plan[\"num_videos\"], plan[\"num_audio\"])\n",
    "    print(f\"Fetched total {len(recs)} assets from providers\")\n",
    "\n",
    "    # 3) preprocess in-memory to tensors / PILs\n",
    "    processed = preprocess_records(recs, image_size=224, audio_duration=5.0, audio_sr=16000)\n",
    "\n",
    "    # 4) prepare ALIGN embedder\n",
    "    align = ALIGNEmbedder(model_name_or_path=align_model_id)\n",
    "\n",
    "    # text embedding for the prompt\n",
    "    txt_emb = align.embed_texts([plan[\"clean_prompt\"]])[0]  # (D,)\n",
    "\n",
    "    # collect image-like inputs (images + video thumbs)\n",
    "    img_inputs = []\n",
    "    img_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] in (\"image\", \"video\") and r.get(\"img_pil\") is not None:\n",
    "            # prefer PIL (processor handles PIL directly)\n",
    "            img_inputs.append(r[\"img_pil\"])\n",
    "            img_indices.append(i)\n",
    "\n",
    "    # embed images\n",
    "    if img_inputs:\n",
    "        print(f\"Embedding {len(img_inputs)} images with ALIGN...\")\n",
    "        img_embs = align.embed_images(img_inputs, batch_size=16)\n",
    "    else:\n",
    "        img_embs = np.zeros((0, align.embedding_dim), dtype=np.float32)\n",
    "\n",
    "    # audio -> mel images\n",
    "    audio_inputs = []\n",
    "    audio_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] == \"audio\" and r.get(\"audio_mel_tensor\") is not None:\n",
    "            # convert CHW float to PIL HWC for processor\n",
    "            mel = r[\"audio_mel_tensor\"]  # CHW float in [-1,1]\n",
    "            # convert to HWC uint8\n",
    "            arr = mel\n",
    "            if arr.ndim == 3 and arr.shape[0] == 3:\n",
    "                arr = np.transpose(arr, (1,2,0))\n",
    "            # arr is float in [-1,1] or [0,1]; scale to 0..255\n",
    "            if arr.max() <= 1.0 + 1e-6 and arr.min() >= -1.0 - 1e-6:\n",
    "                arr = ((arr + 1.0) / 2.0 * 255.0).clip(0,255).astype(np.uint8)\n",
    "            elif arr.max() <= 1.0 + 1e-6 and arr.min() >= 0.0 - 1e-6:\n",
    "                arr = (arr * 255.0).clip(0,255).astype(np.uint8)\n",
    "            else:\n",
    "                arr = arr.clip(0,255).astype(np.uint8)\n",
    "            audio_inputs.append(Image.fromarray(arr))\n",
    "            audio_indices.append(i)\n",
    "\n",
    "    if audio_inputs:\n",
    "        print(f\"Embedding {len(audio_inputs)} audio mel-images with ALIGN...\")\n",
    "        audio_embs = align.embed_images(audio_inputs, batch_size=8)\n",
    "    else:\n",
    "        audio_embs = np.zeros((0, align.embedding_dim), dtype=np.float32)\n",
    "\n",
    "    # 5) compute similarities and rank\n",
    "    results = []\n",
    "    # images:\n",
    "    if img_inputs:\n",
    "        # compute per-image similarity to the single text embedding\n",
    "        sims = (img_embs @ txt_emb) / (np.linalg.norm(img_embs, axis=1) * (np.linalg.norm(txt_emb)+1e-12))\n",
    "        for idx_local, sim in enumerate(sims):\n",
    "            rec_idx = img_indices[idx_local]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"],\n",
    "                \"id\": r[\"id\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"score\": float(sim),\n",
    "                \"url\": r.get(\"url\"),\n",
    "                \"thumbnail\": r.get(\"thumbnail_url\") or None\n",
    "            })\n",
    "    # audio:\n",
    "    if audio_inputs:\n",
    "        sims_a = (audio_embs @ txt_emb) / (np.linalg.norm(audio_embs, axis=1) * (np.linalg.norm(txt_emb)+1e-12))\n",
    "        for idx_local, sim in enumerate(sims_a):\n",
    "            rec_idx = audio_indices[idx_local]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"],\n",
    "                \"id\": r[\"id\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"score\": float(sim),\n",
    "                \"url\": r.get(\"url\"),\n",
    "                \"thumbnail\": r.get(\"thumbnail_url\") or None\n",
    "            })\n",
    "\n",
    "    # sort by score desc and print top_k\n",
    "    results_sorted = sorted(results, key=lambda x: -x[\"score\"])\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, rr in enumerate(results_sorted[:top_k], start=1):\n",
    "        print(f\"{i}. [{rr['type']}] {rr['provider']} id={rr['id']} score={rr['score']:.4f}\")\n",
    "        print(f\"    title: {rr['title']}\")\n",
    "        print(f\"    url  : {rr['url']}\")\n",
    "        print()\n",
    "    return results_sorted\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    raw = input(\"Enter user prompt: \").strip()\n",
    "    if not raw:\n",
    "        raw = \"a happy person running on a beach at sunset with soft warm lighting\"\n",
    "    out = pipeline_run(raw, top_k=10, align_model_id=\"kakaobrain/align-base\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
