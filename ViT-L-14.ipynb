{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc8e183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initial plan: {'clean_prompt': 'girl talking to a man', 'num_images': 5, 'num_videos': 2, 'num_audio': 0, 'notes': 'Images are included to capture the scene, and videos provide dynamic context. No audio is needed as the focus is on the visual interaction.'}\n",
      "[info] Loading ViT-L-14 (OpenCLIP) on cpu — this may download >1.7GB if not cached.\n",
      "Fetched initial 7 assets.\n",
      "\n",
      "=== Iteration 1 === total candidates so far: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 7 items using ViT-L-14...\n",
      "Found 0 assets with score >= 0.2 (threshold).\n",
      "[info] Requesting query refinements from LLM ...\n",
      "Refiner action: refine_query\n",
      "Fetched 5 for query: girl talking to a man close-up\n",
      "Fetched 5 for query: young girl speaking with an adult man\n",
      "Fetched 2 for query: girl having a conversation with a man\n",
      "Fetched 4 for query: child talking to a man\n",
      "\n",
      "=== Iteration 2 === total candidates so far: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 23 items using ViT-L-14...\n",
      "Found 0 assets with score >= 0.2 (threshold).\n",
      "[info] Requesting query refinements from LLM ...\n",
      "Refiner action: refine_query\n",
      "Fetched 0 for query: girl talking to a man close-up\n",
      "Fetched 5 for query: young girl conversing with an adult man\n",
      "Fetched 0 for query: girl and man having a conversation\n",
      "Fetched 4 for query: girl speaking with a man in a casual setting\n",
      "\n",
      "=== Iteration 3 === total candidates so far: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 32 items using ViT-L-14...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 661\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prompt:\n\u001b[32m    660\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33ma girl talking to a man\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m results = \u001b[43miterative_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTHRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_iter_fetch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPER_ITER_FETCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_ITER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 576\u001b[39m, in \u001b[36miterative_pipeline\u001b[39m\u001b[34m(user_prompt, threshold, per_iter_fetch, max_iter, top_k)\u001b[39m\n\u001b[32m    573\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(combined_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m items using ViT-L-14...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m emb_imgs = \u001b[43mvit\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# embed prompt\u001b[39;00m\n\u001b[32m    578\u001b[39m txt_emb = vit.embed_texts([plan[\u001b[33m\"\u001b[39m\u001b[33mclean_prompt\u001b[39m\u001b[33m\"\u001b[39m]])[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 426\u001b[39m, in \u001b[36mViTL14Wrapper.embed_images\u001b[39m\u001b[34m(self, imgs, batch_size)\u001b[39m\n\u001b[32m    424\u001b[39m     t = (t + \u001b[32m1.0\u001b[39m) / \u001b[32m2.0\u001b[39m\n\u001b[32m    425\u001b[39m t = t.type(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m img_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m img_emb = img_emb / img_emb.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    428\u001b[39m embs.append(img_emb.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\model.py:327\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image, normalize)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.normalize(features, dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:909\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    908\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._embeds(x)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m     pooled, tokens = \u001b[38;5;28mself\u001b[39m._pool(x)\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:572\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    570\u001b[39m         x = checkpoint(r, x, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, attn_mask, use_reentrant=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m         x = \u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first:\n\u001b[32m    575\u001b[39m     x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)    \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\open_clip\\transformer.py:299\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, q_x, k_x, v_x, attn_mask)\u001b[39m\n\u001b[32m    297\u001b[39m v_x = \u001b[38;5;28mself\u001b[39m.ln_1_kv(v_x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mln_1_kv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    298\u001b[39m x = q_x + \u001b[38;5;28mself\u001b[39m.ls_1(\u001b[38;5;28mself\u001b[39m.attention(q_x=\u001b[38;5;28mself\u001b[39m.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask))\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ls_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "iterative_vitl14_pipeline.py\n",
    "\n",
    "Single-model pipeline (ViT-L-14 OpenCLIP) + iterative LLM-driven refinement.\n",
    "- Parse user prompt with LLM to produce a search plan (images/videos/audio counts and cleaned query)\n",
    "- Fetch metadata from Pexels (images+videos) and Freesound (audio)\n",
    "- Preprocess thumbnails/previews in-memory -> CLIP-ready tensors\n",
    "- Embed prompt + assets with ViT-L-14 (OpenCLIP) and compute cosine similarity\n",
    "- If not enough assets have score >= threshold, call LLM refinement prompt to suggest query refinements\n",
    "  and fetch more, up to max_iter.\n",
    "\n",
    "Environment variables:\n",
    "  OPENAI_API_KEY, PEXELS_API_KEY, FREESOUND_API_KEY (optional)\n",
    "Optional config via env:\n",
    "  MAX_ITER (default 4), THRESHOLD (default 0.5), PER_ITER_FETCH (default 5)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Optional, Tuple, Set\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# audio libs\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import ffmpeg\n",
    "\n",
    "# ML libs\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Load .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------\n",
    "# Config / env\n",
    "# -------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PEXELS_KEY = os.getenv(\"PEXELS_API_KEY\")\n",
    "FREESOUND_KEY = os.getenv(\"FREESOUND_API_KEY\")\n",
    "\n",
    "MAX_ITER = int(os.getenv(\"MAX_ITER\", \"4\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.2\"))\n",
    "PER_ITER_FETCH = int(os.getenv(\"PER_ITER_FETCH\", \"5\"))\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in environment (or load .env with load_dotenv()).\")\n",
    "if not PEXELS_KEY:\n",
    "    print(\"[warn] No PEXELS_API_KEY set; Pexels fetcher will return empty.\")\n",
    "if not FREESOUND_KEY:\n",
    "    print(\"[warn] No FREESOUND_API_KEY set; Freesound fetcher will return empty.\")\n",
    "\n",
    "# -------------------------\n",
    "# LLM system prompts\n",
    "# -------------------------\n",
    "# Primary planning prompt (one-shot): instruct LLM to return JSON only.\n",
    "PLANNER_SYSTEM_PROMPT = \"\"\"You are a planning assistant that turns an unstructured user prompt\n",
    "into a short, clean search plan for fetching media assets (images, videos, audio).\n",
    "Return JSON ONLY with exactly these keys:\n",
    "- clean_prompt: string (a concise search query suitable for stock APIs)\n",
    "- num_images: integer (0-10)\n",
    "- num_videos: integer (0-10)\n",
    "- num_audio: integer (0-10)\n",
    "- notes: string (brief reason for the choices)\n",
    "\n",
    "Methods to follow:\n",
    "1) Always consider all three media types. Decide which are relevant and allocate counts accordingly.\n",
    "2) If the user explicitly asks for a medium (e.g., 'background music'), favor that medium.\n",
    "3) For visual scenes (people, actions, landscapes), include images and at least 1 video.\n",
    "4) For auditory prompts, include audio (SFX or music).\n",
    "5) Choose counts for diversity (broad prompts -> more results; specific prompts -> fewer).\n",
    "6) Clamp counts to 0..10. Output integers.\n",
    "7) Output JSON only, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "# Refinement prompt: given previous results and which assets were below threshold,\n",
    "# ask the LLM to refine or propose new search queries / modifiers.\n",
    "REFINER_SYSTEM_PROMPT = \"\"\"You are an assistant that refines search queries to improve result relevance.\n",
    "You are given:\n",
    "- the original user prompt\n",
    "- the clean_prompt used previously\n",
    "- a short list of example titles/descriptions (or failures) that had low relevance\n",
    "Your job: provide JSON ONLY with:\n",
    "- action: one of [\"refine_query\",\"expand_query\",\"suggest_filters\",\"stop\"]\n",
    "- queries: list of 1-4 alternative or refined search query strings to try (can include modifiers like \"close-up\", \"studio\", \"child\", \"man speaking\", \"conversation\", \"two people\")\n",
    "- per_query_counts: list of integers same length as queries, indicating how many assets to fetch per query (1-10)\n",
    "- notes: short reasoning\n",
    "\n",
    "Guidelines:\n",
    "- Prefer small semantic changes that increase the chance of matching the intent (e.g. add \"close-up\", specify age/gender if prompt implies it).\n",
    "- If the current results look OK, return action \"stop\".\n",
    "- Output JSON only.\n",
    "\"\"\"\n",
    "\n",
    "# Generic helper: call OpenAI chat completions via HTTP\n",
    "def call_openai_chat(messages: List[Dict], model: str=\"gpt-4o-mini\", max_tokens:int=300, temperature:float=0.0) -> str:\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature, \"max_tokens\": max_tokens}\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # robustly extract assistant content\n",
    "    try:\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        return json.dumps(data)\n",
    "\n",
    "def parse_plan_via_llm(raw_prompt: str) -> Dict:\n",
    "    user = f\"User prompt: {raw_prompt}\\nReturn JSON only.\"\n",
    "    text = call_openai_chat([{\"role\":\"system\",\"content\":PLANNER_SYSTEM_PROMPT},{\"role\":\"user\",\"content\":user}])\n",
    "    # extract JSON blob\n",
    "    if \"{\" in text and \"}\" in text:\n",
    "        try:\n",
    "            s = text.index(\"{\"); e = text.rindex(\"}\") + 1\n",
    "            return json.loads(text[s:e])\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback: try to parse entire text\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        # fallback heuristic\n",
    "        return {\"clean_prompt\": raw_prompt, \"num_images\": 3, \"num_videos\": 1, \"num_audio\": 0, \"notes\":\"fallback heuristics\"}\n",
    "\n",
    "def refine_queries_via_llm(original_prompt: str, prev_clean: str, low_examples: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Ask LLM to refine queries given a list of low-relevance example titles/descriptions.\n",
    "    Returns dict with action, queries(list), per_query_counts(list), notes.\n",
    "    \"\"\"\n",
    "    user_payload = {\n",
    "        \"original_prompt\": original_prompt,\n",
    "        \"prev_clean_prompt\": prev_clean,\n",
    "        \"low_examples\": low_examples[:10]\n",
    "    }\n",
    "    user_text = \"Context (JSON):\\n\" + json.dumps(user_payload) + \"\\n\\nReturn JSON only as specified.\"\n",
    "    text = call_openai_chat([{\"role\":\"system\",\"content\":REFINER_SYSTEM_PROMPT},{\"role\":\"user\",\"content\":user_text}], max_tokens=300)\n",
    "    if \"{\" in text and \"}\" in text:\n",
    "        try:\n",
    "            s = text.index(\"{\"); e = text.rindex(\"}\") + 1\n",
    "            return json.loads(text[s:e])\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        return {\"action\":\"stop\",\"queries\":[],\"per_query_counts\":[],\"notes\":\"fallback stop\"}\n",
    "\n",
    "# -------------------------\n",
    "# Fetchers (Pexels & Freesound) - metadata only\n",
    "# -------------------------\n",
    "def unified_record(provider, id, typ, title, description, url, thumbnail_url=None,\n",
    "                   duration=None, uploader=None, published_at=None, tags=None, raw=None):\n",
    "    return {\n",
    "        \"provider\": provider,\n",
    "        \"id\": str(id),\n",
    "        \"type\": typ,\n",
    "        \"title\": title or \"\",\n",
    "        \"description\": description,\n",
    "        \"url\": url,\n",
    "        \"thumbnail_url\": thumbnail_url,\n",
    "        \"duration\": duration,\n",
    "        \"uploader\": uploader,\n",
    "        \"published_at\": published_at,\n",
    "        \"tags\": tags or [],\n",
    "        \"raw_meta\": raw or {}\n",
    "    }\n",
    "\n",
    "def search_pexels_images(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/v1/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"photos\", []):\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"image\",\n",
    "            title=it.get(\"alt\") or \"\",\n",
    "            description=None,\n",
    "            url=(it.get(\"src\") or {}).get(\"original\"),\n",
    "            thumbnail_url=(it.get(\"src\") or {}).get(\"medium\"),\n",
    "            duration=None,\n",
    "            uploader=it.get(\"photographer\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_pexels_videos(prompt: str, per_page: int = 8) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"videos\", []):\n",
    "        file_url = None\n",
    "        for vf in it.get(\"video_files\", []) or []:\n",
    "            if vf.get(\"quality\") == \"hd\":\n",
    "                file_url = vf.get(\"link\"); break\n",
    "        if not file_url and it.get(\"video_files\"):\n",
    "            file_url = it.get(\"video_files\")[0].get(\"link\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"video\",\n",
    "            title=(it.get(\"user\") or {}).get(\"name\") or str(it.get(\"id\")),\n",
    "            description=it.get(\"url\"),\n",
    "            url=file_url,\n",
    "            thumbnail_url=it.get(\"image\"),\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=(it.get(\"user\") or {}).get(\"name\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_freesound(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not FREESOUND_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://freesound.org/apiv2/search/text/\"\n",
    "    headers = {\"Authorization\": f\"Token {FREESOUND_KEY}\"}\n",
    "    params = {\"query\": prompt, \"page_size\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"results\", []):\n",
    "        preview = (it.get(\"previews\") or {}).get(\"preview-hq-mp3\") or (it.get(\"previews\") or {}).get(\"preview-lq-mp3\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"freesound\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"audio\",\n",
    "            title=it.get(\"name\"),\n",
    "            description=it.get(\"description\"),\n",
    "            url=preview,\n",
    "            thumbnail_url=None,\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=it.get(\"username\"),\n",
    "            published_at=it.get(\"created\"),\n",
    "            tags=it.get(\"tags\") or [],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def fetch_assets_for_query(query: str, num_images:int=0, num_videos:int=0, num_audio:int=0) -> List[Dict]:\n",
    "    results = []\n",
    "    if num_images > 0:\n",
    "        results.extend(search_pexels_images(query, per_page=num_images)[:num_images])\n",
    "    if num_videos > 0:\n",
    "        results.extend(search_pexels_videos(query, per_page=num_videos)[:num_videos])\n",
    "    if num_audio > 0:\n",
    "        results.extend(search_freesound(query, per_page=num_audio)[:num_audio])\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# Download + preprocess (in-memory)\n",
    "# -------------------------\n",
    "def download_bytes(url: str, timeout=20) -> Optional[bytes]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, stream=True)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        # don't spam, just warn\n",
    "        # print(\"[warn] download failed:\", e)\n",
    "        return None\n",
    "\n",
    "def download_image_pil(url: str, timeout=15) -> Optional[Image.Image]:\n",
    "    b = download_bytes(url, timeout=timeout)\n",
    "    if not b:\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def center_crop_and_resize_pil(img: Image.Image, size:int=224) -> Image.Image:\n",
    "    w,h = img.size\n",
    "    m = min(w,h)\n",
    "    left = (w-m)//2\n",
    "    top = (h-m)//2\n",
    "    img = img.crop((left, top, left+m, top+m))\n",
    "    img = img.resize((size,size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def pil_to_clip_tensor(img: Image.Image, size:int=224, normalize:bool=True) -> Optional[np.ndarray]:\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = center_crop_and_resize_pil(img, size=size)\n",
    "    arr = np.asarray(img).astype(np.float32)/255.0  # H W C\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()  # C H W\n",
    "    if normalize:\n",
    "        arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "def decode_audio_bytes_to_waveform(audio_bytes: bytes, target_sr:int=16000, duration:float=5.0) -> Optional[np.ndarray]:\n",
    "    if not audio_bytes:\n",
    "        return None\n",
    "    try:\n",
    "        bio = io.BytesIO(audio_bytes)\n",
    "        data, sr = sf.read(bio, dtype='float32')\n",
    "        if data.ndim > 1:\n",
    "            data = np.mean(data, axis=1)\n",
    "        if sr != target_sr:\n",
    "            data = librosa.resample(data, sr, target_sr)\n",
    "        desired = int(target_sr * duration)\n",
    "        if len(data) > desired:\n",
    "            start = max(0, (len(data)-desired)//2)\n",
    "            data = data[start:start+desired]\n",
    "        elif len(data) < desired:\n",
    "            data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "        return data.astype(np.float32)\n",
    "    except Exception:\n",
    "        try:\n",
    "            proc = (\n",
    "                ffmpeg.input('pipe:0')\n",
    "                .output('pipe:1', format='f32le', ar=target_sr, ac=1)\n",
    "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True)\n",
    "            )\n",
    "            out, err = proc.communicate(input=audio_bytes)\n",
    "            if proc.returncode != 0:\n",
    "                return None\n",
    "            data = np.frombuffer(out, dtype=np.float32)\n",
    "            desired = int(target_sr * duration)\n",
    "            if len(data) > desired:\n",
    "                start = max(0, (len(data)-desired)//2)\n",
    "                data = data[start:start+desired]\n",
    "            elif len(data) < desired:\n",
    "                data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "            return data.astype(np.float32)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def waveform_to_mel_image_tensor(wav: np.ndarray, sr:int=16000, n_mels:int=128, n_fft:int=2048, hop_length:int=512, size:int=224) -> Optional[np.ndarray]:\n",
    "    if wav is None:\n",
    "        return None\n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-9)\n",
    "    img_arr = (S_norm * 255.0).astype(np.uint8)\n",
    "    pil = Image.fromarray(img_arr)\n",
    "    pil = pil.resize((size,size), Image.LANCZOS).convert(\"RGB\")\n",
    "    arr = np.asarray(pil).astype(np.float32)/255.0\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()\n",
    "    arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "def preprocess_records(records: List[Dict], image_size:int=224, audio_duration:float=5.0, audio_sr:int=16000) -> List[Dict]:\n",
    "    processed = []\n",
    "    for r in tqdm(records, desc=\"preprocess\", leave=False):\n",
    "        rec = dict(r)\n",
    "        typ = rec[\"type\"]\n",
    "        if typ == \"image\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"video\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"audio\":\n",
    "            url = rec.get(\"url\")\n",
    "            audio_bytes = download_bytes(url)\n",
    "            wav = decode_audio_bytes_to_waveform(audio_bytes, target_sr=audio_sr, duration=audio_duration)\n",
    "            rec[\"waveform\"] = wav\n",
    "            rec[\"audio_mel_tensor\"] = waveform_to_mel_image_tensor(wav, sr=audio_sr, size=image_size) if wav is not None else None\n",
    "        else:\n",
    "            rec[\"img_tensor\"] = None\n",
    "            rec[\"audio_mel_tensor\"] = None\n",
    "        processed.append(rec)\n",
    "    return processed\n",
    "\n",
    "# -------------------------\n",
    "# ViT-L-14 CLIP wrapper (single model)\n",
    "# -------------------------\n",
    "class ViTL14Wrapper:\n",
    "    def __init__(self, model_name=\"ViT-L-14\", pretrained=\"openai\", device:Optional[str]=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"[info] Loading ViT-L-14 (OpenCLIP) on {self.device} — this may download >1.7GB if not cached.\")\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "        self.model = model.to(self.device).eval()\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        try:\n",
    "            self.text_dim = self.model.text_projection.shape[1]\n",
    "        except Exception:\n",
    "            # fallback guess\n",
    "            self.text_dim = 768\n",
    "        self.dtype = next(self.model.parameters()).dtype\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_images(self, imgs: List[np.ndarray], batch_size:int=8) -> np.ndarray:\n",
    "        \"\"\"imgs: list of numpy arrays (C,H,W) float in [-1,1] or [0,1]\"\"\"\n",
    "        if len(imgs) == 0:\n",
    "            return np.zeros((0, self.model.visual.output_dim), dtype=np.float32)\n",
    "        embs = []\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            t = torch.from_numpy(np.stack(batch, axis=0)).to(self.device)\n",
    "            # if in [-1,1] -> map to [0,1] for model preprocess expectation\n",
    "            if t.max() <= 1.0 + 1e-6 and t.min() >= -1.0 - 1e-6:\n",
    "                t = (t + 1.0) / 2.0\n",
    "            t = t.type(self.dtype)\n",
    "            img_emb = self.model.encode_image(t)\n",
    "            img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "            embs.append(img_emb.cpu().numpy())\n",
    "        return np.vstack(embs).astype(np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts: List[str], batch_size:int=16) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Robust text embedding handling different tokenizer outputs (tensor / list / dict).\n",
    "        \"\"\"\n",
    "        if len(texts) == 0:\n",
    "            return np.zeros((0, self.text_dim), dtype=np.float32)\n",
    "        out_embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            toks = self.tokenizer(chunk)\n",
    "            # Normalize tokenizer output\n",
    "            if isinstance(toks, torch.Tensor):\n",
    "                toks_t = toks.to(self.device)\n",
    "                try:\n",
    "                    txt = self.model.encode_text(toks_t)\n",
    "                except TypeError:\n",
    "                    txt = self.model.encode_text(input_ids=toks_t)\n",
    "            elif isinstance(toks, (list, tuple, np.ndarray)):\n",
    "                toks_t = torch.tensor(toks, device=self.device)\n",
    "                try:\n",
    "                    txt = self.model.encode_text(toks_t)\n",
    "                except TypeError:\n",
    "                    txt = self.model.encode_text(input_ids=toks_t)\n",
    "            elif isinstance(toks, dict):\n",
    "                dict_t = {}\n",
    "                for k,v in toks.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        dict_t[k] = v.to(self.device)\n",
    "                    else:\n",
    "                        try:\n",
    "                            dict_t[k] = torch.tensor(v, device=self.device)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                # if single tensor value -> positional\n",
    "                tensor_vals = [v for v in dict_t.values() if isinstance(v, torch.Tensor)]\n",
    "                if len(tensor_vals) == 1:\n",
    "                    try:\n",
    "                        txt = self.model.encode_text(tensor_vals[0])\n",
    "                    except TypeError:\n",
    "                        txt = self.model.encode_text(input_ids=tensor_vals[0])\n",
    "                else:\n",
    "                    try:\n",
    "                        txt = self.model.encode_text(**dict_t)\n",
    "                    except TypeError:\n",
    "                        txt = self.model.encode_text(tensor_vals[0]) if tensor_vals else torch.zeros((len(chunk), self.text_dim), device=self.device)\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unsupported tokenizer return type: {type(toks)}\")\n",
    "            txt = txt / (txt.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "            out_embs.append(txt.cpu().numpy())\n",
    "        return np.vstack(out_embs).astype(np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: scoring and dedup\n",
    "# -------------------------\n",
    "def cosine_sim_vecs(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # a: (N,d), b: (d,) -> (N,)\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.zeros((a.shape[0],), dtype=np.float32)\n",
    "    bnorm = b / (np.linalg.norm(b) + 1e-12)\n",
    "    an = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    return (an @ bnorm).reshape(-1)\n",
    "\n",
    "def dedupe_records(existing_ids: Set[str], new_records: List[Dict]) -> List[Dict]:\n",
    "    out = []\n",
    "    for r in new_records:\n",
    "        if r[\"provider\"] + \"_\" + r[\"id\"] in existing_ids:\n",
    "            continue\n",
    "        out.append(r)\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Orchestration: iterative fetching + scoring\n",
    "# -------------------------\n",
    "def iterative_pipeline(user_prompt: str, threshold: float = THRESHOLD, per_iter_fetch: int = PER_ITER_FETCH, max_iter:int=MAX_ITER, top_k:int=10):\n",
    "    # 1) initial plan\n",
    "    plan = parse_plan_via_llm(user_prompt)\n",
    "    print(\"LLM initial plan:\", plan)\n",
    "    # instantiate ViT-L-14 model once\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    vit = ViTL14Wrapper(model_name=\"ViT-L-14\", pretrained=\"openai\", device=device)\n",
    "\n",
    "    # Data stores\n",
    "    all_records: List[Dict] = []\n",
    "    seen_ids: Set[str] = set()\n",
    "    iter_count = 0\n",
    "\n",
    "    # helper to fetch from plan or queries list\n",
    "    def fetch_from_plan_entry(clean_q: str, imgs:int, vids:int, auds:int) -> List[Dict]:\n",
    "        recs = fetch_assets_for_query(clean_q, num_images=imgs, num_videos=vids, num_audio=auds)\n",
    "        # unify dedupe by provider_id tag\n",
    "        new = dedupe_records(seen_ids, recs)\n",
    "        for r in new:\n",
    "            seen_ids.add(r[\"provider\"] + \"_\" + r[\"id\"])\n",
    "        return new\n",
    "\n",
    "    # initial batch fetch using plan\n",
    "    initial_recs = fetch_from_plan_entry(plan[\"clean_prompt\"], plan.get(\"num_images\",0), plan.get(\"num_videos\",0), plan.get(\"num_audio\",0))\n",
    "    all_records.extend(initial_recs)\n",
    "    print(f\"Fetched initial {len(initial_recs)} assets.\")\n",
    "\n",
    "    # Main iterative loop\n",
    "    while iter_count < max_iter:\n",
    "        iter_count += 1\n",
    "        print(f\"\\n=== Iteration {iter_count} === total candidates so far: {len(all_records)}\")\n",
    "        # preprocess newly fetched ones (we preprocess all; could optimize)\n",
    "        processed = preprocess_records(all_records, image_size=224)\n",
    "\n",
    "        # build arrays of embeddable image-like tensors\n",
    "        img_tensors = []\n",
    "        img_map_idx = []  # map tensor index -> all_records index\n",
    "        for idx, rec in enumerate(processed):\n",
    "            if rec[\"type\"] in (\"image\",\"video\") and rec.get(\"img_tensor\") is not None:\n",
    "                img_tensors.append(rec[\"img_tensor\"].astype(np.float32))\n",
    "                img_map_idx.append(idx)\n",
    "        # audio -> mel-images also considered as images\n",
    "        audio_tensors = []\n",
    "        audio_map_idx = []\n",
    "        for idx, rec in enumerate(processed):\n",
    "            if rec[\"type\"] == \"audio\" and rec.get(\"audio_mel_tensor\") is not None:\n",
    "                audio_tensors.append(rec[\"audio_mel_tensor\"].astype(np.float32))\n",
    "                audio_map_idx.append(idx)\n",
    "\n",
    "        combined_tensors = img_tensors + audio_tensors\n",
    "        combined_map_idx = img_map_idx + audio_map_idx\n",
    "\n",
    "        if len(combined_tensors) == 0:\n",
    "            print(\"[info] No embeddable items found yet.\")\n",
    "            # attempt to fetch per_iter_fetch new items using the same query\n",
    "            more = fetch_from_plan_entry(plan[\"clean_prompt\"], per_iter_fetch, 0, 0)\n",
    "            if not more:\n",
    "                print(\"[info] No more items available for same query; attempting LLM refinement.\")\n",
    "                ref = refine_queries_via_llm(user_prompt, plan[\"clean_prompt\"], [r.get(\"title\",\"\") for r in all_records])\n",
    "                if ref.get(\"action\",\"stop\") == \"stop\":\n",
    "                    print(\"[info] Refiner suggested stop. Exiting.\")\n",
    "                    break\n",
    "                # else apply queries suggested by ref\n",
    "                for q, c in zip(ref.get(\"queries\",[]), ref.get(\"per_query_counts\",[])):\n",
    "                    new = fetch_from_plan_entry(q, int(c), 0, 0)\n",
    "                    all_records.extend(new)\n",
    "            else:\n",
    "                all_records.extend(more)\n",
    "            continue\n",
    "\n",
    "        print(f\"Embedding {len(combined_tensors)} items using ViT-L-14...\")\n",
    "        emb_imgs = vit.embed_images(combined_tensors, batch_size=8)\n",
    "        # embed prompt\n",
    "        txt_emb = vit.embed_texts([plan[\"clean_prompt\"]])[0]\n",
    "        # compute similarity vector\n",
    "        sims = cosine_sim_vecs(emb_imgs, txt_emb)\n",
    "\n",
    "        # attach scores back to records\n",
    "        for local_i, sim in enumerate(sims):\n",
    "            rec_idx = combined_map_idx[local_i]\n",
    "            all_records[rec_idx].setdefault(\"scores\", {})\n",
    "            all_records[rec_idx][\"scores\"][\"vitl14\"] = float(sim)\n",
    "\n",
    "        # count how many distinct assets exceed threshold\n",
    "        good_records = [r for r in all_records if r.get(\"scores\",{}).get(\"vitl14\", -999) >= threshold]\n",
    "        print(f\"Found {len(good_records)} assets with score >= {threshold} (threshold).\")\n",
    "\n",
    "        # If enough (we might define 'enough' as sum of initially requested counts)\n",
    "        desired_total = plan.get(\"num_images\",0) + plan.get(\"num_videos\",0) + plan.get(\"num_audio\",0)\n",
    "        # but if initial desired_total=0 choose a small target (e.g., 3)\n",
    "        if desired_total <= 0:\n",
    "            desired_total = max(3, per_iter_fetch)\n",
    "\n",
    "        if len(good_records) >= desired_total:\n",
    "            print(f\"Target satisfied: {len(good_records)} >= {desired_total}. Stopping iterations.\")\n",
    "            break\n",
    "\n",
    "        # Otherwise, call the refiner LLM to propose query refinements\n",
    "        low_examples = []\n",
    "        # pick some low-scoring examples to show LLM\n",
    "        sorted_by_score = sorted(all_records, key=lambda r: r.get(\"scores\",{}).get(\"vitl14\", -999), reverse=True)\n",
    "        # list titles of bottom K (or those below threshold)\n",
    "        low = [r for r in sorted_by_score if r.get(\"scores\",{}).get(\"vitl14\", -999) < threshold]\n",
    "        for r in low[:6]:\n",
    "            title = r.get(\"title\") or r.get(\"description\") or \"\"\n",
    "            low_examples.append(title + \" || \" + (r.get(\"thumbnail_url\") or r.get(\"url\") or \"\"))\n",
    "\n",
    "        print(\"[info] Requesting query refinements from LLM ...\")\n",
    "        ref = refine_queries_via_llm(user_prompt, plan[\"clean_prompt\"], low_examples)\n",
    "        action = ref.get(\"action\", \"stop\")\n",
    "        print(\"Refiner action:\", action)\n",
    "        if action == \"stop\":\n",
    "            print(\"[info] Refiner suggested stop or no useful suggestions. Ending.\")\n",
    "            break\n",
    "        # Apply returned queries\n",
    "        queries = ref.get(\"queries\", []) or []\n",
    "        per_counts = ref.get(\"per_query_counts\", []) or []\n",
    "        # pad per_counts if needed\n",
    "        if len(per_counts) < len(queries):\n",
    "            per_counts = per_counts + [per_iter_fetch] * (len(queries)-len(per_counts))\n",
    "        new_found = 0\n",
    "        for q, c in zip(queries, per_counts):\n",
    "            try:\n",
    "                imgs_to_fetch = int(min(max(1, int(c)), 10))\n",
    "            except Exception:\n",
    "                imgs_to_fetch = per_iter_fetch\n",
    "            new_recs = fetch_from_plan_entry(q, imgs_to_fetch, 0, 0)\n",
    "            print(f\"Fetched {len(new_recs)} for query: {q}\")\n",
    "            new_found += len(new_recs)\n",
    "            all_records.extend(new_recs)\n",
    "        if new_found == 0:\n",
    "            print(\"[info] Refiner queries returned no new assets. Ending iterations.\")\n",
    "            break\n",
    "        # optionally update plan.clean_prompt to best refined suggestion\n",
    "        if queries:\n",
    "            plan[\"clean_prompt\"] = queries[0]\n",
    "\n",
    "    # end loop\n",
    "    # Final scoring summary: sort all_records by vitl14 score desc\n",
    "    final_with_scores = [r for r in all_records if r.get(\"scores\",{}).get(\"vitl14\", -999) != -999]\n",
    "    final_sorted = sorted(final_with_scores, key=lambda r: -r[\"scores\"][\"vitl14\"])\n",
    "    print(\"\\nFinal top results (ViT-L-14 scores):\")\n",
    "    for i, rr in enumerate(final_sorted[:top_k], start=1):\n",
    "        print(f\"{i}. [{rr['type']}] {rr['provider']} id={rr['id']} score={rr['scores']['vitl14']:.4f}\")\n",
    "        print(f\"    title: {rr['title']}\")\n",
    "        print(f\"    url  : {rr.get('url')}\")\n",
    "        print()\n",
    "    return final_sorted\n",
    "\n",
    "# -------------------------\n",
    "# CLI entry\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = input(\"Enter user prompt: \").strip()\n",
    "    if not prompt:\n",
    "        prompt = \"a girl talking to a man\"\n",
    "    results = iterative_pipeline(prompt, threshold=THRESHOLD, per_iter_fetch=PER_ITER_FETCH, max_iter=MAX_ITER, top_k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
