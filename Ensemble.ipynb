{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80de2b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: soundfile in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: open-clip-torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (4.14.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: future in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.23.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.28.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.5.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (1.0.20)\n",
      "Requirement already satisfied: filelock in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numba>=0.51.0->librosa) (0.45.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv requests Pillow numpy soundfile librosa ffmpeg-python tqdm open-clip-torch torch transformers sentence-transformers\n",
    "# ensure ffmpeg binary installed on the system (apt/brew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cffae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM plan: {'clean_prompt': 'a girl talking to a man', 'num_images': 5, 'num_videos': 2, 'num_audio': 0, 'notes': 'The prompt is visually focused on a scene involving two people, so I included images and videos to capture the interaction.'}\n",
      "Fetched total 7 assets from providers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|██████████| 7/7 [00:01<00:00,  4.23it/s]\n",
      "c:\\Users\\JENNIFER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JENNIFER\\.cache\\huggingface\\hub\\models--timm--vit_base_patch32_clip_224.openai. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading secondary OpenCLIP model ViT-L-14\n",
      "Embedding 7 images with primary CLIP...\n",
      "\n",
      "Top results (ranked by ensemble_score):\n",
      "1. [image] pexels id=6668312 ensemble=1.0000\n",
      "    primary_score  : 0.2388\n",
      "    secondary_score: 0.1854\n",
      "    title: Father and daughter share a bonding moment reading a storybook indoors.\n",
      "    url  : https://images.pexels.com/photos/6668312/pexels-photo-6668312.jpeg\n",
      "\n",
      "2. [video] pexels id=7426713 ensemble=0.7997\n",
      "    primary_score  : 0.2363\n",
      "    secondary_score: 0.1680\n",
      "    title: Pavel Danilyuk\n",
      "    url  : https://videos.pexels.com/video-files/7426713/7426713-hd_720_1280_25fps.mp4\n",
      "\n",
      "3. [image] pexels id=5710988 ensemble=0.5945\n",
      "    primary_score  : 0.2327\n",
      "    secondary_score: 0.1517\n",
      "    title: A group therapy session indoors with diverse adults in a supportive environment.\n",
      "    url  : https://images.pexels.com/photos/5710988/pexels-photo-5710988.jpeg\n",
      "\n",
      "4. [image] pexels id=5711017 ensemble=0.4032\n",
      "    primary_score  : 0.2305\n",
      "    secondary_score: 0.1350\n",
      "    title: A diverse group of people sitting in a circle during a therapy session in a sports hall.\n",
      "    url  : https://images.pexels.com/photos/5711017/pexels-photo-5711017.jpeg\n",
      "\n",
      "5. [image] pexels id=5710922 ensemble=0.2980\n",
      "    primary_score  : 0.2217\n",
      "    secondary_score: 0.1363\n",
      "    title: A group therapy session with six adults seated in a circle, discussing support and mental health.\n",
      "    url  : https://images.pexels.com/photos/5710922/pexels-photo-5710922.jpeg\n",
      "\n",
      "6. [image] pexels id=5711382 ensemble=0.2359\n",
      "    primary_score  : 0.2192\n",
      "    secondary_score: 0.1334\n",
      "    title: Black and white photo of a support group session indoors with diverse participants.\n",
      "    url  : https://images.pexels.com/photos/5711382/pexels-photo-5711382.jpeg\n",
      "\n",
      "7. [video] pexels id=6209372 ensemble=0.1329\n",
      "    primary_score  : 0.2017\n",
      "    secondary_score: 0.1472\n",
      "    title: cottonbro studio\n",
      "    url  : https://videos.pexels.com/video-files/6209372/6209372-hd_1366_720_25fps.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ensemble_pipeline.py\n",
    "\n",
    "Complete pipeline:\n",
    "- parse prompt with LLM (OpenAI HTTP)\n",
    "- fetch from Pexels (images+videos) and Freesound (audio)\n",
    "- preprocess in-memory to CLIP-ready tensors\n",
    "- embed with primary CLIP (open_clip) and a secondary model (ALIGN or other)\n",
    "- compute per-model similarity to prompt and an ensemble ranking\n",
    "- optional BLIP caption re-rank (disabled by default)\n",
    "\n",
    "Configure via environment variables or .env file:\n",
    "  OPENAI_API_KEY, PEXELS_API_KEY, FREESOUND_API_KEY\n",
    "Optional:\n",
    "  SECOND_MODEL = \"align\" or name like \"ViT-L-14\" (open_clip)\n",
    "  ENABLE_BLIP = \"1\" to enable BLIP caption scoring (requires more downloads)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# audio libs\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import ffmpeg\n",
    "\n",
    "# ML libs\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# optional HF models for ALIGN or others\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoModel, AutoTokenizer\n",
    "# BLIP (optional)\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load .env if present\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------\n",
    "# Config / env\n",
    "# -------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PEXELS_KEY = os.getenv(\"PEXELS_API_KEY\")\n",
    "FREESOUND_KEY = os.getenv(\"FREESOUND_API_KEY\")\n",
    "SECOND_MODEL = os.getenv(\"SECOND_MODEL\", \"ViT-L-14\")  # use \"align\" to attempt HF align\n",
    "ENABLE_BLIP = os.getenv(\"ENABLE_BLIP\", \"0\") == \"1\"\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in environment (or load .env with load_dotenv()).\")\n",
    "if not PEXELS_KEY:\n",
    "    print(\"[warn] No PEXELS_API_KEY set; Pexels fetcher will return empty.\")\n",
    "if not FREESOUND_KEY:\n",
    "    print(\"[warn] No FREESOUND_API_KEY set; Freesound fetcher will return empty.\")\n",
    "\n",
    "# -------------------------\n",
    "# LLM prompt parser (OpenAI HTTP)\n",
    "# -------------------------\n",
    "_SYSTEM_PROMPT = \"\"\"You are an assistant that converts an unstructured user prompt into a structured search plan for media assets (images, videos, audio). Always consider all three asset types and decide which are useful for the prompt.\n",
    "\n",
    "Your output MUST be valid JSON only with exactly these fields:\n",
    "- clean_prompt: (string) a cleaned, concise query suitable for search engines like Pexels, Unsplash, or Freesound\n",
    "- num_images: (integer) number of image assets to fetch (0–10)\n",
    "- num_videos: (integer) number of video assets to fetch (0–10)\n",
    "- num_audio: (integer) number of audio assets to fetch (0–10)\n",
    "- notes: (string) brief reasoning why you chose these asset types and counts\n",
    "\n",
    "Guidelines (methods you MUST follow):\n",
    "1. Parse the user's intent. If the user explicitly mentions a medium (e.g., 'background music'), allocate more to that medium.\n",
    "2. If the prompt is visually focused (people, landscapes, actions), include images and consider videos (default at least 1 video for visual scenes).\n",
    "3. If the prompt implies sound (birds chirping, music, crowd noise), include audio (music, SFX, ambience).\n",
    "4. Decide numbers based on diversity: broad/ambiguous prompts -> larger counts (up to 10); very specific prompts -> fewer (1–4).\n",
    "5. Keep results practical: use integers, clamp counts to 0..10.\n",
    "6. Output JSON ONLY. Do NOT include extra text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "def _clamp_int(v, lo=0, hi=10):\n",
    "    try:\n",
    "        vi = int(v)\n",
    "    except Exception:\n",
    "        return lo\n",
    "    if vi < lo: return lo\n",
    "    if vi > hi: return hi\n",
    "    return vi\n",
    "\n",
    "def parse_prompt_with_openai_http(raw_prompt: str) -> Dict:\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    user_message = f\"User prompt: {raw_prompt}\\n\\nReturn JSON only.\"\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",   # change if needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": _SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 300,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        text = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # extract JSON blob robustly\n",
    "        if \"{\" in text and \"}\" in text:\n",
    "            start = text.index(\"{\")\n",
    "            end = text.rindex(\"}\") + 1\n",
    "            json_str = text[start:end]\n",
    "            j = json.loads(json_str)\n",
    "        else:\n",
    "            j = json.loads(text)\n",
    "        clean_prompt = str(j.get(\"clean_prompt\") or raw_prompt).strip()\n",
    "        num_images = _clamp_int(j.get(\"num_images\", 0))\n",
    "        num_videos = _clamp_int(j.get(\"num_videos\", 0))\n",
    "        num_audio  = _clamp_int(j.get(\"num_audio\", 0))\n",
    "        notes = str(j.get(\"notes\") or \"\").strip()\n",
    "        if (num_images + num_videos + num_audio) == 0:\n",
    "            low = raw_prompt.lower()\n",
    "            if any(w in low for w in (\"music\",\"song\",\"sound\",\"audio\",\"ambient\",\"sfx\",\"effect\")):\n",
    "                num_audio = max(2, num_audio)\n",
    "            if any(w in low for w in (\"photo\",\"image\",\"picture\",\"video\",\"clip\",\"scene\",\"shot\",\"portrait\",\"landscape\")):\n",
    "                num_images = max(3, num_images); num_videos = max(1, num_videos)\n",
    "            if (num_images + num_videos + num_audio) == 0:\n",
    "                num_images = 3; num_videos = 1\n",
    "        return {\n",
    "            \"clean_prompt\": clean_prompt,\n",
    "            \"num_images\": num_images,\n",
    "            \"num_videos\": num_videos,\n",
    "            \"num_audio\": num_audio,\n",
    "            \"notes\": notes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"[warn] parse prompt failed:\", e)\n",
    "        raw = raw_prompt.strip()\n",
    "        low = raw.lower()\n",
    "        if any(w in low for w in (\"music\",\"song\",\"sound\",\"audio\",\"ambient\",\"sfx\",\"effect\")):\n",
    "            return {\"clean_prompt\": raw, \"num_images\":0, \"num_videos\":0, \"num_audio\":6, \"notes\":\"fallback audio\"}\n",
    "        else:\n",
    "            return {\"clean_prompt\": raw, \"num_images\":5, \"num_videos\":2, \"num_audio\":1, \"notes\":\"fallback visual\"}\n",
    "\n",
    "# -------------------------\n",
    "# Fetchers: Pexels & Freesound (metadata only + thumbnail/preview URLs)\n",
    "# -------------------------\n",
    "def unified_record(provider, id, typ, title, description, url, thumbnail_url=None,\n",
    "                   duration=None, uploader=None, published_at=None, tags=None, raw=None):\n",
    "    return {\n",
    "        \"provider\": provider,\n",
    "        \"id\": str(id),\n",
    "        \"type\": typ,  # image|video|audio\n",
    "        \"title\": title or \"\",\n",
    "        \"description\": description,\n",
    "        \"url\": url,\n",
    "        \"thumbnail_url\": thumbnail_url,\n",
    "        \"duration\": duration,\n",
    "        \"uploader\": uploader,\n",
    "        \"published_at\": published_at,\n",
    "        \"tags\": tags or [],\n",
    "        \"raw_meta\": raw or {}\n",
    "    }\n",
    "\n",
    "def search_pexels_images(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/v1/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"photos\", []):\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"image\",\n",
    "            title=it.get(\"alt\") or \"\",\n",
    "            description=None,\n",
    "            url=(it.get(\"src\") or {}).get(\"original\"),\n",
    "            thumbnail_url=(it.get(\"src\") or {}).get(\"medium\"),\n",
    "            duration=None,\n",
    "            uploader=it.get(\"photographer\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_pexels_videos(prompt: str, per_page: int = 8) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"videos\", []):\n",
    "        file_url = None\n",
    "        for vf in it.get(\"video_files\", []) or []:\n",
    "            if vf.get(\"quality\") == \"hd\":\n",
    "                file_url = vf.get(\"link\"); break\n",
    "        if not file_url and it.get(\"video_files\"):\n",
    "            file_url = it.get(\"video_files\")[0].get(\"link\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"video\",\n",
    "            title=(it.get(\"user\") or {}).get(\"name\") or str(it.get(\"id\")),\n",
    "            description=it.get(\"url\"),\n",
    "            url=file_url,\n",
    "            thumbnail_url=it.get(\"image\"),\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=(it.get(\"user\") or {}).get(\"name\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def search_freesound(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not FREESOUND_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://freesound.org/apiv2/search/text/\"\n",
    "    headers = {\"Authorization\": f\"Token {FREESOUND_KEY}\"}\n",
    "    params = {\"query\": prompt, \"page_size\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"results\", []):\n",
    "        preview = (it.get(\"previews\") or {}).get(\"preview-hq-mp3\") or (it.get(\"previews\") or {}).get(\"preview-lq-mp3\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"freesound\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"audio\",\n",
    "            title=it.get(\"name\"),\n",
    "            description=it.get(\"description\"),\n",
    "            url=preview,\n",
    "            thumbnail_url=None,\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=it.get(\"username\"),\n",
    "            published_at=it.get(\"created\"),\n",
    "            tags=it.get(\"tags\") or [],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "def fetch_assets(clean_prompt: str, num_images:int, num_videos:int, num_audio:int) -> List[Dict]:\n",
    "    results = []\n",
    "    if num_images > 0:\n",
    "        imgs = search_pexels_images(clean_prompt, per_page=num_images)\n",
    "        results.extend(imgs[:num_images])\n",
    "    if num_videos > 0:\n",
    "        vids = search_pexels_videos(clean_prompt, per_page=num_videos)\n",
    "        results.extend(vids[:num_videos])\n",
    "    if num_audio > 0:\n",
    "        aud = search_freesound(clean_prompt, per_page=num_audio)\n",
    "        results.extend(aud[:num_audio])\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# In-memory download & preprocessing\n",
    "# -------------------------\n",
    "def download_bytes(url: str, timeout=20) -> Optional[bytes]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, stream=True)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        print(\"[warn] download failed:\", e)\n",
    "        return None\n",
    "\n",
    "def download_image_pil(url: str, timeout=15) -> Optional[Image.Image]:\n",
    "    b = download_bytes(url, timeout=timeout)\n",
    "    if not b:\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"[warn] PIL open failed:\", e)\n",
    "        return None\n",
    "\n",
    "def center_crop_and_resize_pil(img: Image.Image, size:int=224) -> Image.Image:\n",
    "    w,h = img.size\n",
    "    m = min(w,h)\n",
    "    left = (w-m)//2\n",
    "    top = (h-m)//2\n",
    "    img = img.crop((left, top, left+m, top+m))\n",
    "    img = img.resize((size,size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def pil_to_clip_tensor(img: Image.Image, size:int=224, normalize:bool=True) -> Optional[np.ndarray]:\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = center_crop_and_resize_pil(img, size=size)\n",
    "    arr = np.asarray(img).astype(np.float32)/255.0  # H W C\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()  # C H W\n",
    "    if normalize:\n",
    "        arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "def decode_audio_bytes_to_waveform(audio_bytes: bytes, target_sr:int=16000, duration:float=5.0) -> Optional[np.ndarray]:\n",
    "    if not audio_bytes:\n",
    "        return None\n",
    "    try:\n",
    "        bio = io.BytesIO(audio_bytes)\n",
    "        data, sr = sf.read(bio, dtype='float32')\n",
    "        if data.ndim > 1:\n",
    "            data = np.mean(data, axis=1)\n",
    "        if sr != target_sr:\n",
    "            data = librosa.resample(data, sr, target_sr)\n",
    "        desired = int(target_sr * duration)\n",
    "        if len(data) > desired:\n",
    "            start = max(0, (len(data)-desired)//2)\n",
    "            data = data[start:start+desired]\n",
    "        elif len(data) < desired:\n",
    "            data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "        return data.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            proc = (\n",
    "                ffmpeg.input('pipe:0')\n",
    "                .output('pipe:1', format='f32le', ar=target_sr, ac=1)\n",
    "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True)\n",
    "            )\n",
    "            out, err = proc.communicate(input=audio_bytes)\n",
    "            if proc.returncode != 0:\n",
    "                raise RuntimeError(\"ffmpeg decode fail\")\n",
    "            data = np.frombuffer(out, dtype=np.float32)\n",
    "            desired = int(target_sr * duration)\n",
    "            if len(data) > desired:\n",
    "                start = max(0, (len(data)-desired)//2)\n",
    "                data = data[start:start+desired]\n",
    "            elif len(data) < desired:\n",
    "                data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "            return data.astype(np.float32)\n",
    "        except Exception as e2:\n",
    "            print(\"[warn] audio decode failed:\", e, e2)\n",
    "            return None\n",
    "\n",
    "def waveform_to_mel_image_tensor(wav: np.ndarray, sr:int=16000, n_mels:int=128, n_fft:int=2048, hop_length:int=512, size:int=224) -> Optional[np.ndarray]:\n",
    "    if wav is None:\n",
    "        return None\n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-9)\n",
    "    img_arr = (S_norm * 255.0).astype(np.uint8)\n",
    "    pil = Image.fromarray(img_arr)\n",
    "    pil = pil.resize((size,size), Image.LANCZOS).convert(\"RGB\")\n",
    "    arr = np.asarray(pil).astype(np.float32)/255.0\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()\n",
    "    arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "def preprocess_records(records: List[Dict], image_size:int=224, audio_duration:float=5.0, audio_sr:int=16000) -> List[Dict]:\n",
    "    processed = []\n",
    "    for r in tqdm(records, desc=\"preprocess\"):\n",
    "        rec = dict(r)\n",
    "        typ = rec[\"type\"]\n",
    "        if typ == \"image\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"video\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"audio\":\n",
    "            url = rec.get(\"url\")\n",
    "            audio_bytes = download_bytes(url)\n",
    "            wav = decode_audio_bytes_to_waveform(audio_bytes, target_sr=audio_sr, duration=audio_duration)\n",
    "            rec[\"waveform\"] = wav\n",
    "            rec[\"audio_mel_tensor\"] = waveform_to_mel_image_tensor(wav, sr=audio_sr, size=image_size) if wav is not None else None\n",
    "        else:\n",
    "            rec[\"img_tensor\"] = None\n",
    "            rec[\"audio_mel_tensor\"] = None\n",
    "        processed.append(rec)\n",
    "    return processed\n",
    "\n",
    "# -------------------------\n",
    "# Embedding wrappers: primary CLIP (open_clip) and secondary model\n",
    "# -------------------------\n",
    "class CLIPWrapper:\n",
    "    def __init__(self, model_name=\"ViT-B-32\", pretrained=\"openai\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "        self.model = model.to(self.device).eval()\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        self.dtype = next(self.model.parameters()).dtype\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_images(self, imgs: List[np.ndarray], batch_size:int=32) -> np.ndarray:\n",
    "        if len(imgs) == 0:\n",
    "            return np.zeros((0, self.model.visual.output_dim), dtype=np.float32)\n",
    "        embs = []\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            t = torch.from_numpy(np.stack(batch, axis=0)).to(self.device)\n",
    "            if t.max() <= 1.0 + 1e-6 and t.min() >= -1.0 - 1e-6:\n",
    "                t = (t + 1.0) / 2.0\n",
    "            t = t.type(self.dtype)\n",
    "            img_emb = self.model.encode_image(t)\n",
    "            img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "            embs.append(img_emb.cpu().numpy())\n",
    "        return np.vstack(embs).astype(np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Robust text embedding that handles different tokenizer return types:\n",
    "          - torch.LongTensor -> call model.encode_text(tensor)\n",
    "          - list/ndarray -> convert to torch.LongTensor and call model.encode_text(tensor)\n",
    "          - dict -> try to convert values to tensors and:\n",
    "             * if dict has single tensor-like value -> call model.encode_text(tensor)\n",
    "             * else try model.encode_text(**dict) and fallback to passing first tensor\n",
    "        Returns numpy array (N, D), L2-normalized.\n",
    "        \"\"\"\n",
    "        if len(texts) == 0:\n",
    "            try:\n",
    "                text_dim = self.model.text_projection.shape[1]\n",
    "            except Exception:\n",
    "                text_dim = 512\n",
    "            return np.zeros((0, text_dim), dtype=np.float32)\n",
    "\n",
    "        out_embs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            toks = self.tokenizer(chunk)  # may be tensor, list, ndarray, or dict\n",
    "\n",
    "            # Normalize into torch tensors on device\n",
    "            toks_norm = None\n",
    "            if isinstance(toks, torch.Tensor):\n",
    "                toks_norm = toks.to(self.device)\n",
    "                # call positional\n",
    "                try:\n",
    "                    txt_emb = self.model.encode_text(toks_norm)\n",
    "                except TypeError:\n",
    "                    # some models expect kwargs, try as dict\n",
    "                    try:\n",
    "                        txt_emb = self.model.encode_text(input_ids=toks_norm)\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(f\"encode_text failed for tensor tokenizer output: {e}\")\n",
    "            elif isinstance(toks, (list, tuple, np.ndarray)):\n",
    "                # convert to tensor\n",
    "                toks_tensor = torch.tensor(toks, device=self.device)\n",
    "                try:\n",
    "                    txt_emb = self.model.encode_text(toks_tensor)\n",
    "                except TypeError:\n",
    "                    try:\n",
    "                        txt_emb = self.model.encode_text(input_ids=toks_tensor)\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(f\"encode_text failed for list/ndarray tokenizer output: {e}\")\n",
    "            elif isinstance(toks, dict):\n",
    "                # convert values to tensors if needed\n",
    "                dict_t = {}\n",
    "                for k, v in toks.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        dict_t[k] = v.to(self.device)\n",
    "                    else:\n",
    "                        try:\n",
    "                            dict_t[k] = torch.tensor(v, device=self.device)\n",
    "                        except Exception:\n",
    "                            # fallback: skip this key\n",
    "                            pass\n",
    "\n",
    "                # If dict has exactly one tensor, pass it positionally\n",
    "                tensor_values = [v for v in dict_t.values() if isinstance(v, torch.Tensor)]\n",
    "                if len(tensor_values) == 1:\n",
    "                    single = tensor_values[0]\n",
    "                    try:\n",
    "                        txt_emb = self.model.encode_text(single)\n",
    "                    except TypeError:\n",
    "                        # try with kwarg name guesses\n",
    "                        try:\n",
    "                            txt_emb = self.model.encode_text(input_ids=single)\n",
    "                        except Exception as e:\n",
    "                            raise RuntimeError(f\"encode_text failed for single-token dict: {e}\")\n",
    "                else:\n",
    "                    # try calling with kwargs; if that fails, try passing first tensor positional\n",
    "                    try:\n",
    "                        txt_emb = self.model.encode_text(**dict_t)\n",
    "                    except TypeError:\n",
    "                        if tensor_values:\n",
    "                            first = tensor_values[0]\n",
    "                            try:\n",
    "                                txt_emb = self.model.encode_text(first)\n",
    "                            except Exception as e:\n",
    "                                raise RuntimeError(f\"encode_text failed for dict fallback: {e}\")\n",
    "                        else:\n",
    "                            raise RuntimeError(\"Tokenizer returned dict but no tensor-like values found.\")\n",
    "            else:\n",
    "                raise RuntimeError(f\"Unsupported tokenizer output type: {type(toks)}\")\n",
    "\n",
    "            # normalize and append\n",
    "            txt_emb = txt_emb / (txt_emb.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "            out_embs.append(txt_emb.cpu().numpy())\n",
    "\n",
    "        return np.vstack(out_embs).astype(np.float32)\n",
    "\n",
    "# Secondary model loader: try ALIGN via HF if requested, otherwise load another open_clip model name\n",
    "class SecondaryWrapper:\n",
    "    def __init__(self, mode=\"align\", device=None):\n",
    "        self.mode = mode.lower()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.available = False\n",
    "        # try ALIGN via HF if mode == \"align\"\n",
    "        if self.mode == \"align\":\n",
    "            try:\n",
    "                # We'll attempt to load a vision-text model if available; if not, fallback.\n",
    "                # Note: ALIGN HF checkpoint interfaces differ; this is a best-effort attempt.\n",
    "                print(\"[info] Attempting to load ALIGN via HF 'kakaobrain/align-base' (may download large files)...\")\n",
    "                self.processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
    "                self.model_hf = AutoModel.from_pretrained(\"kakaobrain/align-base\").to(self.device).eval()\n",
    "                # For text embedding, we need tokenizer — try AutoTokenizer\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"kakaobrain/align-base\")\n",
    "                self.available = True\n",
    "                print(\"[info] ALIGN model loaded.\")\n",
    "            except Exception as e:\n",
    "                print(\"[warn] ALIGN load failed:\", e)\n",
    "                self.available = False\n",
    "        # fallback: load another open_clip variant if mode is like 'ViT-L-14' or fallback\n",
    "        if not self.available:\n",
    "            try:\n",
    "                model_name = mode if mode and mode.lower() != \"align\" else \"ViT-L-14\"\n",
    "                print(f\"[info] Loading secondary OpenCLIP model {model_name}\")\n",
    "                self.clip = CLIPWrapper(model_name=model_name, pretrained=\"openai\", device=self.device)\n",
    "                self.available = True\n",
    "            except Exception as e:\n",
    "                print(\"[warn] secondary open_clip load failed:\", e)\n",
    "                self.available = False\n",
    "\n",
    "    def embed_images(self, imgs: List[np.ndarray], batch_size=16) -> np.ndarray:\n",
    "        if not self.available:\n",
    "            return np.zeros((len(imgs), 512), dtype=np.float32)\n",
    "        if hasattr(self, \"clip\"):\n",
    "            return self.clip.embed_images(imgs, batch_size=batch_size)\n",
    "        else:\n",
    "            # best-effort HF path: convert PIL? we won't implement complex logic here\n",
    "            raise NotImplementedError(\"HF ALIGN image embedding path not implemented in this script.\")\n",
    "\n",
    "    def embed_texts(self, texts: List[str], batch_size=16) -> np.ndarray:\n",
    "        if not self.available:\n",
    "            return np.zeros((len(texts), 512), dtype=np.float32)\n",
    "        if hasattr(self, \"clip\"):\n",
    "            return self.clip.embed_texts(texts, batch_size=batch_size)\n",
    "        else:\n",
    "            raise NotImplementedError(\"HF ALIGN text embedding path not implemented in this script.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# BLIP captioner (optional)\n",
    "# -------------------------\n",
    "class BLIPCaptioner:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        try:\n",
    "            self.proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "            self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(self.device)\n",
    "            self.model.eval()\n",
    "            self.available = True\n",
    "        except Exception as e:\n",
    "            print(\"[warn] BLIP load failed:\", e)\n",
    "            self.available = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def caption(self, pil_images: List[Image.Image], max_length:int=30) -> List[str]:\n",
    "        if not self.available:\n",
    "            return [\"\" for _ in pil_images]\n",
    "        caps = []\n",
    "        for img in pil_images:\n",
    "            if img is None:\n",
    "                caps.append(\"\")\n",
    "                continue\n",
    "            inputs = self.proc(images=img, return_tensors=\"pt\").to(self.device)\n",
    "            out = self.model.generate(**inputs, max_new_tokens=max_length)\n",
    "            txt = self.proc.decode(out[0], skip_special_tokens=True)\n",
    "            caps.append(txt)\n",
    "        return caps\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: scoring and fusion\n",
    "# -------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # a: (N,d), b: (M,d) -> (N,M)\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.zeros((a.shape[0], b.shape[0]), dtype=np.float32)\n",
    "    an = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    bn = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "    return an.dot(bn.T)\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    if x.size == 0:\n",
    "        return x\n",
    "    mn, mx = x.min(), x.max()\n",
    "    if mx <= mn:\n",
    "        return np.ones_like(x) * 0.5\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "def fuse_minmax_average(score_list: List[np.ndarray]) -> np.ndarray:\n",
    "    # score_list: list of 1D arrays of same length\n",
    "    if not score_list:\n",
    "        return np.array([])\n",
    "    nm = [minmax_normalize(s) for s in score_list]\n",
    "    stacked = np.vstack(nm)  # (models, N)\n",
    "    return stacked.mean(axis=0)\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline orchestration\n",
    "# -------------------------\n",
    "def pipeline_run(user_prompt: str, top_k:int=10):\n",
    "    plan = parse_prompt_with_openai_http(user_prompt)\n",
    "    print(\"LLM plan:\", plan)\n",
    "\n",
    "    recs = fetch_assets(plan[\"clean_prompt\"], plan[\"num_images\"], plan[\"num_videos\"], plan[\"num_audio\"])\n",
    "    print(f\"Fetched total {len(recs)} assets from providers\")\n",
    "\n",
    "    processed = preprocess_records(recs, image_size=224, audio_duration=5.0, audio_sr=16000)\n",
    "\n",
    "    # instantiate models\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_primary = CLIPWrapper(model_name=\"ViT-B-32\", pretrained=\"openai\", device=device)\n",
    "    clip_secondary = SecondaryWrapper(mode=SECOND_MODEL, device=device)\n",
    "    blip = BLIPCaptioner(device=device) if ENABLE_BLIP else None\n",
    "\n",
    "    # embed prompt text using both models\n",
    "    txt_emb_primary = clip_primary.embed_texts([plan[\"clean_prompt\"]])[0]\n",
    "    txt_emb_secondary = clip_secondary.embed_texts([plan[\"clean_prompt\"]])[0] if clip_secondary.available else None\n",
    "\n",
    "    # prepare image tensors and indices\n",
    "    img_tensors = []\n",
    "    img_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] in (\"image\",\"video\") and r.get(\"img_tensor\") is not None:\n",
    "            img_tensors.append(r[\"img_tensor\"].astype(np.float32))\n",
    "            img_indices.append(i)\n",
    "\n",
    "    # embed images\n",
    "    if img_tensors:\n",
    "        print(f\"Embedding {len(img_tensors)} images with primary CLIP...\")\n",
    "        img_embs_primary = clip_primary.embed_images(img_tensors, batch_size=16)\n",
    "        img_embs_secondary = clip_secondary.embed_images(img_tensors, batch_size=16) if clip_secondary.available else np.zeros_like(img_embs_primary)\n",
    "    else:\n",
    "        img_embs_primary = np.zeros((0, txt_emb_primary.shape[0]))\n",
    "        img_embs_secondary = np.zeros((0, txt_emb_primary.shape[0])) if txt_emb_primary is not None else np.zeros((0,512))\n",
    "\n",
    "    # audio embedding via mel-image fallback through CLIP (primary/secondary)\n",
    "    audio_tensors = []\n",
    "    audio_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] == \"audio\" and r.get(\"audio_mel_tensor\") is not None:\n",
    "            audio_tensors.append(r[\"audio_mel_tensor\"].astype(np.float32))\n",
    "            audio_indices.append(i)\n",
    "    if audio_tensors:\n",
    "        print(f\"Embedding {len(audio_tensors)} audio mel-images with primary CLIP...\")\n",
    "        audio_embs_primary = clip_primary.embed_images(audio_tensors, batch_size=8)\n",
    "        audio_embs_secondary = clip_secondary.embed_images(audio_tensors, batch_size=8) if clip_secondary.available else np.zeros_like(audio_embs_primary)\n",
    "    else:\n",
    "        audio_embs_primary = np.zeros((0, txt_emb_primary.shape[0]))\n",
    "        audio_embs_secondary = np.zeros((0, txt_emb_primary.shape[0]))\n",
    "\n",
    "    # compute similarities\n",
    "    results = []\n",
    "    # images\n",
    "    if img_tensors:\n",
    "        sims_p = (img_embs_primary @ txt_emb_primary) / (np.linalg.norm(img_embs_primary,axis=1) * (np.linalg.norm(txt_emb_primary)+1e-12))\n",
    "        sims_s = (img_embs_secondary @ txt_emb_secondary) / (np.linalg.norm(img_embs_secondary,axis=1) * (np.linalg.norm(txt_emb_secondary)+1e-12)) if txt_emb_secondary is not None else np.zeros_like(sims_p)\n",
    "        for local_idx, (sp, ss) in enumerate(zip(sims_p, sims_s)):\n",
    "            rec_idx = img_indices[local_idx]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"], \"id\": r[\"id\"], \"type\": r[\"type\"], \"title\": r[\"title\"],\n",
    "                \"primary_score\": float(sp), \"secondary_score\": float(ss),\n",
    "                \"url\": r.get(\"url\"), \"thumbnail\": r.get(\"thumbnail_url\")\n",
    "            })\n",
    "    # audio\n",
    "    if audio_tensors:\n",
    "        sims_p = (audio_embs_primary @ txt_emb_primary) / (np.linalg.norm(audio_embs_primary,axis=1) * (np.linalg.norm(txt_emb_primary)+1e-12))\n",
    "        sims_s = (audio_embs_secondary @ txt_emb_secondary) / (np.linalg.norm(audio_embs_secondary,axis=1) * (np.linalg.norm(txt_emb_secondary)+1e-12)) if txt_emb_secondary is not None else np.zeros_like(sims_p)\n",
    "        for local_idx, (sp, ss) in enumerate(zip(sims_p, sims_s)):\n",
    "            rec_idx = audio_indices[local_idx]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"], \"id\": r[\"id\"], \"type\": r[\"type\"], \"title\": r[\"title\"],\n",
    "                \"primary_score\": float(sp), \"secondary_score\": float(ss),\n",
    "                \"url\": r.get(\"url\"), \"thumbnail\": r.get(\"thumbnail_url\")\n",
    "            })\n",
    "\n",
    "    if not results:\n",
    "        print(\"[info] No embeddable results found. Exiting.\")\n",
    "        return []\n",
    "\n",
    "    # build arrays and ensemble\n",
    "    prim = np.array([r[\"primary_score\"] for r in results], dtype=np.float32)\n",
    "    sec  = np.array([r[\"secondary_score\"] for r in results], dtype=np.float32)\n",
    "    fused = fuse_minmax_average([prim, sec])  # average of minmax-normalized scores\n",
    "\n",
    "    # Add fused & ranking\n",
    "    for i, r in enumerate(results):\n",
    "        r[\"secondary_score\"] = float(sec[i])\n",
    "        r[\"primary_score\"] = float(prim[i])\n",
    "        r[\"ensemble_score\"] = float(fused[i])\n",
    "\n",
    "    # optional BLIP captions & caption similarity (optional stronger re-rank)\n",
    "    if ENABLE_BLIP and blip and blip.available:\n",
    "        # caption all items that have pil\n",
    "        pil_images = []\n",
    "        map_idx_to_result = []\n",
    "        for i_res, r in enumerate(results):\n",
    "            # find corresponding processed record to get pil image\n",
    "            # find first processed with matching id\n",
    "            found = None\n",
    "            for pr in processed:\n",
    "                if pr[\"id\"] == r[\"id\"]:\n",
    "                    found = pr\n",
    "                    break\n",
    "            pil = found.get(\"img_pil\") if found else None\n",
    "            pil_images.append(pil)\n",
    "            map_idx_to_result.append(i_res)\n",
    "        captions = blip.caption(pil_images)\n",
    "        # embed captions via primary CLIP text encoder and compute similarity\n",
    "        cap_embs = clip_primary.embed_texts(captions)\n",
    "        prompt_emb = clip_primary.embed_texts([plan[\"clean_prompt\"]])[0]\n",
    "        cap_sims = (cap_embs @ prompt_emb) / (np.linalg.norm(cap_embs,axis=1) * (np.linalg.norm(prompt_emb)+1e-12))\n",
    "        # normalize and combine into ensemble (replace previous ensemble or combine further)\n",
    "        cap_nm = minmax_normalize(cap_sims)\n",
    "        fused_with_caption = (fused + cap_nm) / 2.0\n",
    "        for i_r, r in enumerate(results):\n",
    "            r[\"blip_caption\"] = captions[i_r]\n",
    "            r[\"caption_sim\"] = float(cap_sims[i_r])\n",
    "            r[\"ensemble_score_caption\"] = float(fused_with_caption[i_r])\n",
    "        # present ranked by ensemble_with_caption\n",
    "        rank_key = \"ensemble_score_caption\"\n",
    "    else:\n",
    "        rank_key = \"ensemble_score\"\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda x: -x.get(rank_key, 0.0))\n",
    "\n",
    "    # print top_k\n",
    "    print(\"\\nTop results (ranked by {}):\".format(rank_key))\n",
    "    for i, rr in enumerate(results_sorted[:top_k], start=1):\n",
    "        print(f\"{i}. [{rr['type']}] {rr['provider']} id={rr['id']} ensemble={rr.get('ensemble_score'):.4f}\")\n",
    "        print(f\"    primary_score  : {rr.get('primary_score'):.4f}\")\n",
    "        print(f\"    secondary_score: {rr.get('secondary_score'):.4f}\")\n",
    "        if \"caption_sim\" in rr:\n",
    "            print(f\"    caption_sim    : {rr.get('caption_sim'):.4f}\")\n",
    "        print(f\"    title: {rr['title']}\")\n",
    "        print(f\"    url  : {rr['url']}\")\n",
    "        print()\n",
    "\n",
    "    return results_sorted\n",
    "\n",
    "# -------------------------\n",
    "# CLI entry\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = input(\"Enter user prompt: \").strip()\n",
    "    if not prompt:\n",
    "        prompt = \"a girl talking to a man\"\n",
    "    out = pipeline_run(prompt, top_k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d77085",
   "metadata": {},
   "source": [
    "Model Comparison: OpenCLIP Primary vs Secondary\n",
    "\n",
    "Primary Model: ViT-B-32 (OpenCLIP)\n",
    "\n",
    "Parameters: ~150 million\n",
    "\n",
    "Image Resolution: 224×224\n",
    "\n",
    "Strengths: Lightweight, fast inference, low memory usage, and provides decent general relevance across a wide range of prompts.\n",
    "\n",
    "Weaknesses: Lower semantic depth — it struggles with complex relationships or emotional/contextual compositions (e.g., human interactions, nuanced scenes).\n",
    "\n",
    "Secondary Model: ViT-L-14 (OpenCLIP)\n",
    "\n",
    "Parameters: ~428 million\n",
    "\n",
    "Image Resolution: 224×224 or 336×336\n",
    "\n",
    "Strengths: Much larger embedding space, captures compositional and contextual understanding better, and aligns more strongly with text semantics.\n",
    "\n",
    "Weaknesses: Requires over 1.7 GB of model weights, needs significantly more GPU/CPU memory, and runs slower than ViT-B-32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b8f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
