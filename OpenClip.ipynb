{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25dd76a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: soundfile in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: open-clip-torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: future in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.23.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.28.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (0.5.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from open-clip-torch) (1.0.20)\n",
      "Requirement already satisfied: filelock in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numba>=0.51.0->librosa) (0.45.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from timm>=1.0.17->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jennifer\\appdata\\roaming\\python\\python313\\site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai requests python-dotenv Pillow numpy soundfile librosa ffmpeg-python tqdm open-clip-torch torch\n",
    "# System-level: ffmpeg must be installed on the machine (apt install ffmpeg / brew install ffmpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471220a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ace28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM plan: {'clean_prompt': 'a girl talking to a man', 'num_images': 5, 'num_videos': 0, 'num_audio': 0, 'notes': ''}\n",
      "Fetched total 5 assets from providers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess: 100%|██████████| 5/5 [00:01<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 5 images with CLIP...\n",
      "\n",
      "Top results:\n",
      "1. [image] pexels id=6668312 score=0.2388\n",
      "    title: Father and daughter share a bonding moment reading a storybook indoors.\n",
      "    url  : https://images.pexels.com/photos/6668312/pexels-photo-6668312.jpeg\n",
      "\n",
      "2. [image] pexels id=5710988 score=0.2327\n",
      "    title: A group therapy session indoors with diverse adults in a supportive environment.\n",
      "    url  : https://images.pexels.com/photos/5710988/pexels-photo-5710988.jpeg\n",
      "\n",
      "3. [image] pexels id=5711017 score=0.2305\n",
      "    title: A diverse group of people sitting in a circle during a therapy session in a sports hall.\n",
      "    url  : https://images.pexels.com/photos/5711017/pexels-photo-5711017.jpeg\n",
      "\n",
      "4. [image] pexels id=5710922 score=0.2217\n",
      "    title: A group therapy session with six adults seated in a circle, discussing support and mental health.\n",
      "    url  : https://images.pexels.com/photos/5710922/pexels-photo-5710922.jpeg\n",
      "\n",
      "5. [image] pexels id=5711382 score=0.2192\n",
      "    title: Black and white photo of a support group session indoors with diverse participants.\n",
      "    url  : https://images.pexels.com/photos/5711382/pexels-photo-5711382.jpeg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "llm_fetch_clip_pipeline.py\n",
    "\n",
    "End-to-end script:\n",
    "1) parse user prompt with OpenAI LLM to determine: cleaned prompt, #assets, which types (image/video/audio)\n",
    "2) fetch metadata from Pexels (images + videos) and Freesound (audio) accordingly\n",
    "3) download thumbnails/previews in-memory and preprocess into CLIP-ready tensors\n",
    "4) embed prompt and assets using OpenCLIP and compute cosine similarity (relevance)\n",
    "5) print top-K per type\n",
    "\n",
    "Environment variables:\n",
    "  OPENAI_API_KEY, PEXELS_API_KEY, FREESOUND_API_KEY\n",
    "\n",
    "NOTE: This script does not save files to disk.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from tqdm import tqdm\n",
    "\n",
    "# audio libs\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import ffmpeg\n",
    "\n",
    "# openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# open_clip\n",
    "try:\n",
    "    import open_clip\n",
    "    _OPENCLIP_AVAILABLE = True\n",
    "except Exception:\n",
    "    _OPENCLIP_AVAILABLE = False\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PEXELS_KEY = os.getenv(\"PEXELS_API_KEY\")\n",
    "FREESOUND_KEY = os.getenv(\"FREESOUND_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Set OPENAI_API_KEY in env\")\n",
    "if not PEXELS_KEY:\n",
    "    print(\"[WARN] No PEXELS_API_KEY found; Pexels fetching will return empty.\")\n",
    "if not FREESOUND_KEY:\n",
    "    print(\"[WARN] No FREESOUND_API_KEY found; Freesound fetching will return empty.\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# -------------------------\n",
    "# 1) LLM prompt parser (OpenAI)\n",
    "# -------------------------\n",
    "client = OpenAI()\n",
    "\n",
    "def parse_prompt_with_openai(raw_prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses the new openai client API:\n",
    "      client.chat.completions.create(...)\n",
    "    Returns a dict with clean_prompt, num_images, num_videos, num_audio, notes\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a helpful assistant that converts an unstructured user prompt \"\n",
    "        \"into a structured search plan. Return JSON only with fields: clean_prompt (string), \"\n",
    "        \"num_images (int), num_videos (int), num_audio (int), notes (string). \"\n",
    "        \"Rules:\\n\"\n",
    "        \"- If the user didn't request videos, set num_videos to 0.\\n\"\n",
    "        \"- If the user didn't request audio, set num_audio to 0.\\n\"\n",
    "        \"- Make counts sensible (1-10 per type). Be conservative if uncertain.\\n\"\n",
    "        \"- Clean the prompt for image/video/audio search (no extra commentary).\"\n",
    "    )\n",
    "    user = f\"User prompt: {raw_prompt}\\n\\nReturn JSON only.\"\n",
    "\n",
    "    # call new client\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   # replace if unavailable\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    # The new response shape: resp.choices[0].message.content\n",
    "    text = \"\"\n",
    "    try:\n",
    "        text = resp.choices[0].message[\"content\"]\n",
    "    except Exception:\n",
    "        # fallback: sometimes it's resp.choices[0].message.content\n",
    "        try:\n",
    "            text = resp.choices[0].message.content\n",
    "        except Exception:\n",
    "            text = str(resp)\n",
    "\n",
    "    # parse JSON\n",
    "    try:\n",
    "        j = json.loads(text)\n",
    "        return {\n",
    "            \"clean_prompt\": j.get(\"clean_prompt\") or raw_prompt,\n",
    "            \"num_images\": int(j.get(\"num_images\") or 0),\n",
    "            \"num_videos\": int(j.get(\"num_videos\") or 0),\n",
    "            \"num_audio\": int(j.get(\"num_audio\") or 0),\n",
    "            \"notes\": j.get(\"notes\",\"\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # fallback heuristic if parsing fails\n",
    "        print(\"[warn] OpenAI response not parseable as JSON; falling back. Raw text:\", text[:400])\n",
    "        return {\n",
    "            \"clean_prompt\": raw_prompt,\n",
    "            \"num_images\": 5,\n",
    "            \"num_videos\": 0,\n",
    "            \"num_audio\": 0,\n",
    "            \"notes\": \"fallback heuristic used\"\n",
    "        }# -------------------------\n",
    "# 2) Fetchers (Pexels & Freesound)\n",
    "# -------------------------\n",
    "def unified_record(provider, id, typ, title, description, url, thumbnail_url=None,\n",
    "                   duration=None, uploader=None, published_at=None, tags=None, raw=None):\n",
    "    return {\n",
    "        \"provider\": provider,\n",
    "        \"id\": str(id),\n",
    "        \"type\": typ,  # image|video|audio\n",
    "        \"title\": title or \"\",\n",
    "        \"description\": description,\n",
    "        \"url\": url,\n",
    "        \"thumbnail_url\": thumbnail_url,\n",
    "        \"duration\": duration,\n",
    "        \"uploader\": uploader,\n",
    "        \"published_at\": published_at,\n",
    "        \"tags\": tags or [],\n",
    "        \"raw_meta\": raw or {}\n",
    "    }\n",
    "\n",
    "# Pexels images\n",
    "def search_pexels_images(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/v1/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"photos\", []):\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"image\",\n",
    "            title=it.get(\"alt\") or \"\",\n",
    "            description=None,\n",
    "            url=(it.get(\"src\") or {}).get(\"original\"),\n",
    "            thumbnail_url=(it.get(\"src\") or {}).get(\"medium\"),\n",
    "            duration=None,\n",
    "            uploader=it.get(\"photographer\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "# Pexels videos\n",
    "def search_pexels_videos(prompt: str, per_page: int = 8) -> List[Dict]:\n",
    "    if not PEXELS_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://api.pexels.com/videos/search\"\n",
    "    headers = {\"Authorization\": PEXELS_KEY}\n",
    "    params = {\"query\": prompt, \"per_page\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"videos\", []):\n",
    "        file_url = None\n",
    "        for vf in it.get(\"video_files\", []) or []:\n",
    "            if vf.get(\"quality\") == \"hd\":\n",
    "                file_url = vf.get(\"link\"); break\n",
    "        if not file_url and it.get(\"video_files\"):\n",
    "            file_url = it.get(\"video_files\")[0].get(\"link\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"pexels\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"video\",\n",
    "            title=(it.get(\"user\") or {}).get(\"name\") or str(it.get(\"id\")),\n",
    "            description=it.get(\"url\"),\n",
    "            url=file_url,\n",
    "            thumbnail_url=it.get(\"image\"),\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=(it.get(\"user\") or {}).get(\"name\"),\n",
    "            published_at=None,\n",
    "            tags=[],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "# Freesound\n",
    "def search_freesound(prompt: str, per_page: int = 10) -> List[Dict]:\n",
    "    if not FREESOUND_KEY:\n",
    "        return []\n",
    "    endpoint = \"https://freesound.org/apiv2/search/text/\"\n",
    "    headers = {\"Authorization\": f\"Token {FREESOUND_KEY}\"}\n",
    "    params = {\"query\": prompt, \"page_size\": per_page}\n",
    "    r = requests.get(endpoint, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    out = []\n",
    "    for it in r.json().get(\"results\", []):\n",
    "        preview = (it.get(\"previews\") or {}).get(\"preview-hq-mp3\") or (it.get(\"previews\") or {}).get(\"preview-lq-mp3\")\n",
    "        out.append(unified_record(\n",
    "            provider=\"freesound\",\n",
    "            id=it.get(\"id\"),\n",
    "            typ=\"audio\",\n",
    "            title=it.get(\"name\"),\n",
    "            description=it.get(\"description\"),\n",
    "            url=preview,\n",
    "            thumbnail_url=None,\n",
    "            duration=it.get(\"duration\"),\n",
    "            uploader=it.get(\"username\"),\n",
    "            published_at=it.get(\"created\"),\n",
    "            tags=it.get(\"tags\") or [],\n",
    "            raw=it\n",
    "        ))\n",
    "    return out\n",
    "\n",
    "# Fetch orchestration\n",
    "def fetch_assets(clean_prompt: str, num_images:int, num_videos:int, num_audio:int) -> List[Dict]:\n",
    "    results = []\n",
    "    if num_images > 0:\n",
    "        imgs = search_pexels_images(clean_prompt, per_page=num_images)\n",
    "        results.extend(imgs[:num_images])\n",
    "    if num_videos > 0:\n",
    "        vids = search_pexels_videos(clean_prompt, per_page=num_videos)\n",
    "        results.extend(vids[:num_videos])\n",
    "    if num_audio > 0:\n",
    "        aud = search_freesound(clean_prompt, per_page=num_audio)\n",
    "        results.extend(aud[:num_audio])\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# 3) In-memory download & preprocessing to CLIP-ready tensors\n",
    "# -------------------------\n",
    "def download_bytes(url: str, timeout=20) -> Optional[bytes]:\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, stream=True)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "    except Exception as e:\n",
    "        print(\"[warn] download failed:\", e)\n",
    "        return None\n",
    "\n",
    "def download_image_pil(url: str, timeout=15) -> Optional[Image.Image]:\n",
    "    b = download_bytes(url, timeout=timeout)\n",
    "    if not b:\n",
    "        return None\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(\"[warn] PIL open failed:\", e)\n",
    "        return None\n",
    "\n",
    "def center_crop_and_resize_pil(img: Image.Image, size:int=224) -> Image.Image:\n",
    "    w,h = img.size\n",
    "    m = min(w,h)\n",
    "    left = (w-m)//2\n",
    "    top = (h-m)//2\n",
    "    img = img.crop((left, top, left+m, top+m))\n",
    "    img = img.resize((size,size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def pil_to_clip_tensor(img: Image.Image, size:int=224, normalize:bool=True) -> Optional[np.ndarray]:\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = center_crop_and_resize_pil(img, size=size)\n",
    "    arr = np.asarray(img).astype(np.float32)/255.0  # H W C\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()  # C H W\n",
    "    if normalize:\n",
    "        arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "# audio decode & mel -> 3-channel image-like tensor\n",
    "def decode_audio_bytes_to_waveform(audio_bytes: bytes, target_sr:int=16000, duration:float=5.0) -> Optional[np.ndarray]:\n",
    "    if not audio_bytes:\n",
    "        return None\n",
    "    try:\n",
    "        bio = io.BytesIO(audio_bytes)\n",
    "        data, sr = sf.read(bio, dtype='float32')\n",
    "        if data.ndim > 1:\n",
    "            data = np.mean(data, axis=1)\n",
    "        if sr != target_sr:\n",
    "            data = librosa.resample(data, sr, target_sr)\n",
    "        desired = int(target_sr * duration)\n",
    "        if len(data) > desired:\n",
    "            start = max(0, (len(data)-desired)//2)\n",
    "            data = data[start:start+desired]\n",
    "        elif len(data) < desired:\n",
    "            data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "        return data.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        # fallback: ffmpeg\n",
    "        try:\n",
    "            proc = (\n",
    "                ffmpeg.input('pipe:0')\n",
    "                .output('pipe:1', format='f32le', ar=target_sr, ac=1)\n",
    "                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True)\n",
    "            )\n",
    "            out, err = proc.communicate(input=audio_bytes)\n",
    "            if proc.returncode != 0:\n",
    "                raise RuntimeError(\"ffmpeg decode fail\")\n",
    "            data = np.frombuffer(out, dtype=np.float32)\n",
    "            desired = int(target_sr * duration)\n",
    "            if len(data) > desired:\n",
    "                start = max(0, (len(data)-desired)//2)\n",
    "                data = data[start:start+desired]\n",
    "            elif len(data) < desired:\n",
    "                data = np.concatenate([data, np.zeros(desired - len(data), dtype=np.float32)])\n",
    "            return data.astype(np.float32)\n",
    "        except Exception as e2:\n",
    "            print(\"[warn] audio decode failed:\", e, e2)\n",
    "            return None\n",
    "\n",
    "def waveform_to_mel_image_tensor(wav: np.ndarray, sr:int=16000, n_mels:int=128, n_fft:int=2048, hop_length:int=512, size:int=224) -> Optional[np.ndarray]:\n",
    "    if wav is None:\n",
    "        return None\n",
    "    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-9)\n",
    "    img_arr = (S_norm * 255.0).astype(np.uint8)  # H(n_mels) x T\n",
    "    pil = Image.fromarray(img_arr)\n",
    "    pil = pil.resize((size,size), Image.LANCZOS).convert(\"RGB\")\n",
    "    arr = np.asarray(pil).astype(np.float32)/255.0\n",
    "    arr = np.transpose(arr,(2,0,1)).copy()\n",
    "    arr = arr*2.0 - 1.0\n",
    "    return arr\n",
    "\n",
    "# preprocess a list of records in-memory\n",
    "def preprocess_records(records: List[Dict], image_size:int=224, audio_duration:float=5.0, audio_sr:int=16000) -> List[Dict]:\n",
    "    processed = []\n",
    "    for r in tqdm(records, desc=\"preprocess\"):\n",
    "        rec = dict(r)\n",
    "        typ = rec[\"type\"]\n",
    "        if typ == \"image\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"video\":\n",
    "            url = rec.get(\"thumbnail_url\") or rec.get(\"url\")\n",
    "            pil = download_image_pil(url)\n",
    "            rec[\"img_pil\"] = pil\n",
    "            rec[\"img_tensor\"] = pil_to_clip_tensor(pil, size=image_size) if pil is not None else None\n",
    "        elif typ == \"audio\":\n",
    "            url = rec.get(\"url\")\n",
    "            audio_bytes = download_bytes(url)\n",
    "            wav = decode_audio_bytes_to_waveform(audio_bytes, target_sr=audio_sr, duration=audio_duration)\n",
    "            rec[\"waveform\"] = wav\n",
    "            rec[\"audio_mel_tensor\"] = waveform_to_mel_image_tensor(wav, sr=audio_sr, size=image_size) if wav is not None else None\n",
    "        else:\n",
    "            rec[\"img_tensor\"] = None\n",
    "            rec[\"audio_mel_tensor\"] = None\n",
    "        processed.append(rec)\n",
    "    return processed\n",
    "\n",
    "# -------------------------\n",
    "# 4) CLIP embedding & scoring (OpenCLIP)\n",
    "# -------------------------\n",
    "# Replace your previous CLIPEmbedder with this fixed implementation.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import open_clip\n",
    "from typing import List, Optional\n",
    "\n",
    "class CLIPEmbedder:\n",
    "    def __init__(self, model_name: str = \"ViT-B-32\", pretrained: str = \"openai\", device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Correctly initializes OpenCLIP model, preprocess transform, and text tokenizer.\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # create model and transforms; open_clip returns (model, _, preprocess) in many versions\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)\n",
    "        self.model = model.to(self.device).eval()\n",
    "        self.preprocess = preprocess  # torchvision transform for PIL images\n",
    "        # get text tokenizer separately (this returns a callable tokenizer)\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        # keep dtype\n",
    "        self.dtype = next(self.model.parameters()).dtype\n",
    "        # helper dims (text projection dim may be on model.text_projection)\n",
    "        try:\n",
    "            self.image_dim = self.model.visual.output_dim\n",
    "        except Exception:\n",
    "            # fallback size\n",
    "            self.image_dim = 512\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_images(self, imgs: List[np.ndarray], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        imgs: list of numpy arrays shape (3,H,W), float32 in [-1,1] or [0,1]\n",
    "        returns: numpy array (N, D) normalized (unit)\n",
    "        \"\"\"\n",
    "        if len(imgs) == 0:\n",
    "            return np.zeros((0, self.image_dim), dtype=np.float32)\n",
    "\n",
    "        embs_list = []\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            batch = imgs[i:i+batch_size]\n",
    "            # convert each numpy (3,H,W) to torch tensor (C,H,W) and then stack -> (B,C,H,W)\n",
    "            t = torch.from_numpy(np.stack(batch, axis=0)).to(self.device)\n",
    "            # If values are in [-1,1], convert to [0,1] since open_clip preprocess expects 0..1 images (then normalization)\n",
    "            if t.max() <= 1.0 + 1e-6 and t.min() >= -1.0 - 1e-6:\n",
    "                t = (t + 1.0) / 2.0\n",
    "            # ensure float dtype matches model dtype\n",
    "            t = t.type(self.dtype)\n",
    "            # encode\n",
    "            img_emb = self.model.encode_image(t)\n",
    "            img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "            embs_list.append(img_emb.cpu().numpy())\n",
    "        return np.vstack(embs_list).astype(np.float32)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        texts: list of strings\n",
    "        returns: numpy array (N, D) normalized (unit)\n",
    "        Handles different tokenizer return types:\n",
    "          - dict of torch tensors (best case)\n",
    "          - dict of lists/ndarrays (convert to torch)\n",
    "          - single tensor/list/ndarray (wrap as {'input_ids': tensor})\n",
    "        \"\"\"\n",
    "        if len(texts) == 0:\n",
    "            try:\n",
    "                text_dim = self.model.text_projection.shape[1]\n",
    "            except Exception:\n",
    "                text_dim = 512\n",
    "            return np.zeros((0, text_dim), dtype=np.float32)\n",
    "\n",
    "        embs_list = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "\n",
    "            # Tokenize using tokenizer (may return dict or tensor or list)\n",
    "            toks = self.tokenizer(chunk)\n",
    "\n",
    "            # Normalize token output to a dict of torch tensors on device\n",
    "            if isinstance(toks, dict):\n",
    "                toks_norm = {}\n",
    "                for k, v in toks.items():\n",
    "                    # v could be torch.Tensor already or list/numpy\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        toks_norm[k] = v.to(self.device)\n",
    "                    else:\n",
    "                        toks_norm[k] = torch.tensor(v, device=self.device)\n",
    "            else:\n",
    "                # toks is not a dict (could be torch.Tensor of shape (B, L) or list of lists)\n",
    "                if isinstance(toks, torch.Tensor):\n",
    "                    toks_norm = {\"input_ids\": toks.to(self.device)}\n",
    "                else:\n",
    "                    # assume list/ndarray -> convert to tensor\n",
    "                    toks_norm = {\"input_ids\": torch.tensor(toks, device=self.device)}\n",
    "\n",
    "            # Some open_clip versions expect 'text' key instead of 'input_ids' or expect specific names.\n",
    "            # If model.encode_text errors, try these fallbacks in order.\n",
    "            try:\n",
    "                txt_emb = self.model.encode_text(**toks_norm)\n",
    "            except TypeError:\n",
    "                # Try remove unexpected keys, keep only tensor-like keys\n",
    "                allowed = {k: v for k,v in toks_norm.items() if isinstance(v, torch.Tensor)}\n",
    "                try:\n",
    "                    txt_emb = self.model.encode_text(**allowed)\n",
    "                except Exception as e:\n",
    "                    # Last fallback: if tokens are just input_ids, try pass as single arg\n",
    "                    if \"input_ids\" in allowed:\n",
    "                        try:\n",
    "                            txt_emb = self.model.encode_text(allowed[\"input_ids\"])\n",
    "                        except Exception as e2:\n",
    "                            raise RuntimeError(f\"encode_text failed with fallbacks: {e} / {e2}\")\n",
    "                    else:\n",
    "                        raise RuntimeError(f\"encode_text failed and no known fallback available: {e}\")\n",
    "\n",
    "            txt_emb = txt_emb / (txt_emb.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "            embs_list.append(txt_emb.cpu().numpy())\n",
    "        return np.vstack(embs_list).astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_sim_matrix(img_embs: np.ndarray, txt_embs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity matrix (N_img, N_txt)\n",
    "        \"\"\"\n",
    "        if img_embs.size == 0 or txt_embs.size == 0:\n",
    "            return np.zeros((img_embs.shape[0], txt_embs.shape[0]), dtype=np.float32)\n",
    "        a = img_embs / (np.linalg.norm(img_embs, axis=1, keepdims=True) + 1e-12)\n",
    "        b = txt_embs / (np.linalg.norm(txt_embs, axis=1, keepdims=True) + 1e-12)\n",
    "        return a.dot(b.T)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Pipeline orchestration\n",
    "# -------------------------\n",
    "def pipeline_run(user_prompt: str, top_k:int=5):\n",
    "    # 1) parse prompt via OpenAI LLM\n",
    "    plan = parse_prompt_with_openai(user_prompt)\n",
    "    print(\"LLM plan:\", plan)\n",
    "\n",
    "    # 2) fetch assets according to plan\n",
    "    recs = fetch_assets(plan[\"clean_prompt\"], plan[\"num_images\"], plan[\"num_videos\"], plan[\"num_audio\"])\n",
    "    print(f\"Fetched total {len(recs)} assets from providers\")\n",
    "\n",
    "    # 3) preprocess in-memory to tensors\n",
    "    processed = preprocess_records(recs, image_size=224, audio_duration=5.0, audio_sr=16000)\n",
    "\n",
    "    # 4) prepare arrays for CLIP\n",
    "    clip = CLIPEmbedder(model_name=\"ViT-B-32\", pretrained=\"openai\")\n",
    "    # text embedding for the prompt\n",
    "    txt_emb = clip.embed_texts([plan[\"clean_prompt\"]])[0]\n",
    "\n",
    "    # collect image tensors (images + video thumbs)\n",
    "    img_tensors = []\n",
    "    img_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] in (\"image\",\"video\") and r.get(\"img_tensor\") is not None:\n",
    "            img_tensors.append(r[\"img_tensor\"].astype(np.float32))\n",
    "            img_indices.append(i)\n",
    "\n",
    "    # embed images\n",
    "    if img_tensors:\n",
    "        print(f\"Embedding {len(img_tensors)} images with CLIP...\")\n",
    "        img_embs = clip.embed_images(img_tensors, batch_size=16)\n",
    "    else:\n",
    "        img_embs = np.zeros((0,512))\n",
    "\n",
    "    # audio via mel-image fallback through CLIP (if any)\n",
    "    audio_tensors = []\n",
    "    audio_indices = []\n",
    "    for i, r in enumerate(processed):\n",
    "        if r[\"type\"] == \"audio\" and r.get(\"audio_mel_tensor\") is not None:\n",
    "            audio_tensors.append(r[\"audio_mel_tensor\"].astype(np.float32))\n",
    "            audio_indices.append(i)\n",
    "    if audio_tensors:\n",
    "        print(f\"Embedding {len(audio_tensors)} audio mel-images with CLIP...\")\n",
    "        audio_embs = clip.embed_images(audio_tensors, batch_size=8)\n",
    "    else:\n",
    "        audio_embs = np.zeros((0, img_embs.shape[1] if img_embs.size else 512))\n",
    "\n",
    "    # 5) compute similarities and rank\n",
    "    results = []\n",
    "    # images:\n",
    "    if img_tensors:\n",
    "        sims = (img_embs @ txt_emb) / (np.linalg.norm(img_embs, axis=1) * (np.linalg.norm(txt_emb)+1e-12))\n",
    "        for idx_local, sim in enumerate(sims):\n",
    "            rec_idx = img_indices[idx_local]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"],\n",
    "                \"id\": r[\"id\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"score\": float(sim),\n",
    "                \"url\": r.get(\"url\"),\n",
    "                \"thumbnail\": r.get(\"thumbnail_url\") or None\n",
    "            })\n",
    "    # audio:\n",
    "    if audio_tensors:\n",
    "        sims_a = (audio_embs @ txt_emb) / (np.linalg.norm(audio_embs, axis=1) * (np.linalg.norm(txt_emb)+1e-12))\n",
    "        for idx_local, sim in enumerate(sims_a):\n",
    "            rec_idx = audio_indices[idx_local]\n",
    "            r = processed[rec_idx]\n",
    "            results.append({\n",
    "                \"provider\": r[\"provider\"],\n",
    "                \"id\": r[\"id\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"score\": float(sim),\n",
    "                \"url\": r.get(\"url\"),\n",
    "                \"thumbnail\": r.get(\"thumbnail_url\") or None\n",
    "            })\n",
    "\n",
    "    # sort by score desc and print top_k\n",
    "    results_sorted = sorted(results, key=lambda x: -x[\"score\"])\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, rr in enumerate(results_sorted[:top_k], start=1):\n",
    "        print(f\"{i}. [{rr['type']}] {rr['provider']} id={rr['id']} score={rr['score']:.4f}\")\n",
    "        print(f\"    title: {rr['title']}\")\n",
    "        print(f\"    url  : {rr['url']}\")\n",
    "        print()\n",
    "\n",
    "    return results_sorted\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    raw = input(\"Enter user prompt: \").strip()\n",
    "    if not raw:\n",
    "        raw = \"a happy person running on a beach at sunset with soft warm lighting\"\n",
    "    out = pipeline_run(raw, top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f104e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==0.28) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==0.28) (3.11.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.20->openai==0.28) (2025.8.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->openai==0.28) (1.19.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 2.0.0\n",
      "    Uninstalling openai-2.0.0:\n",
      "      Successfully uninstalled openai-2.0.0\n",
      "Successfully installed openai-0.28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13d22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a1f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman (align vs clip): 0.09999999999999999\n",
      "Kendall tau: 0.0\n",
      "5710922 align_rank 0 clip_rank 3 rank_diff -3\n",
      "5711017 align_rank 1 clip_rank 2 rank_diff -1\n",
      "6668312 align_rank 2 clip_rank 0 rank_diff 2\n",
      "5710988 align_rank 3 clip_rank 1 rank_diff 2\n",
      "5711382 align_rank 4 clip_rank 4 rank_diff 0\n"
     ]
    }
   ],
   "source": [
    "# given lists (top->bottom) from each model\n",
    "align_ids = [\"5710922\",\"5711017\",\"6668312\",\"5710988\",\"5711382\"]\n",
    "clip_ids  = [\"6668312\",\"5710988\",\"5711017\",\"5710922\",\"5711382\"]\n",
    "\n",
    "# 1) Spearman rank correlation\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "spearman_corr, _ = spearmanr([align_ids.index(i) for i in align_ids],\n",
    "                             [clip_ids.index(i) for i in align_ids])\n",
    "kendall_corr, _ = kendalltau([align_ids.index(i) for i in align_ids],\n",
    "                             [clip_ids.index(i) for i in align_ids])\n",
    "print(\"Spearman (align vs clip):\", spearman_corr)\n",
    "print(\"Kendall tau:\", kendall_corr)\n",
    "\n",
    "# 2) simple normalized-rank difference per item\n",
    "def norm_rank_score(rank, n):\n",
    "    return 1.0 - (rank / (n-1))  # 1 for top, 0 for bottom\n",
    "n = len(align_ids)\n",
    "for i in align_ids:\n",
    "    a_rank = align_ids.index(i)\n",
    "    c_rank = clip_ids.index(i)\n",
    "    print(i, \"align_rank\", a_rank, \"clip_rank\", c_rank, \"rank_diff\", a_rank-c_rank)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
