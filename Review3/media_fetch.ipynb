{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8647619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 1000 (587.7 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293a3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.42.0\n",
      "Uninstalling openai-1.42.0:\n",
      "  Successfully uninstalled openai-1.42.0\n",
      "Found existing installation: httpx 0.28.1\n",
      "Uninstalling httpx-0.28.1:\n",
      "  Successfully uninstalled httpx-0.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y openai httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2a156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.35.13\n",
      "  Downloading openai-1.35.13-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: httpx==0.27.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.27.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.35.13) (4.14.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx==0.27.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx==0.27.0) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx==0.27.0) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx==0.27.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.35.13) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.35.13) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.35.13) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai==1.35.13) (0.4.6)\n",
      "Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.42.0\n",
      "    Uninstalling openai-1.42.0:\n",
      "      Successfully uninstalled openai-1.42.0\n",
      "Successfully installed openai-1.35.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"openai==1.35.13\" \"httpx==0.27.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35402ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.42.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.42.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==1.42.0) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.42.0) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.42.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.42.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.42.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.42.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.42.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.42.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jennifer\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai==1.42.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# PROMPT UNDERSTANDING MODULE (Step 1 of Media Fetching Framework)\n",
    "# ===============================================================\n",
    "\n",
    "!pip install openai==1.42.0 pandas matplotlib seaborn\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# =========================\n",
    "\n",
    "# üîß SETUP\n",
    "# =========================\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))# Helper function for OpenAI call (new SDK syntax)\n",
    "def ask_llm(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a reasoning engine that analyzes creative prompts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        max_tokens=700\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_llm(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a reasoning engine that analyzes creative prompts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "        max_tokens=700\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f594255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Intent Extraction Result:\n",
      " {\n",
      "  \"modality\": \"photo\",\n",
      "  \"domain\": \"natural\",\n",
      "  \"primary_subject\": \"cup of coffee\",\n",
      "  \"context_scene\": \"rainy morning window sill\",\n",
      "  \"style_adjectives\": []\n",
      "}\n",
      "\n",
      "üéØ Realism & Abstractness Scoring:\n",
      " {\n",
      "  \"realism_score\": 0.9,\n",
      "  \"abstractness_score\": 0.1,\n",
      "  \"rationale\": \"The subject of a cup of coffee is highly plausible in the context of a rainy morning on a window sill, as it evokes a common and relatable scene. There are no modifiers present to introduce abstraction, reinforcing the realism of the scenario. Overall, this concept is very realistic and lacks imaginative elements.\"\n",
      "}\n",
      "\n",
      "üåê Feasibility Judgement:\n",
      " {\n",
      "  \"feasibility_label\": \"feasible\",\n",
      "  \"realism_overall_score\": 0.9,\n",
      "  \"creative_potential_score\": 0.1,\n",
      "  \"summary\": \"The scenario of a cup of coffee on a rainy morning window sill is highly plausible and relatable, making it feasible for image generation. The high realism score reflects its grounded nature, while the low creative potential indicates a lack of imaginative elements.\"\n",
      "}\n",
      "\n",
      "üó∫Ô∏è Semantic Map:\n",
      "\n",
      "{\n",
      "  \"user_prompt\": \"A simple photo of a cup of coffee on a rainy morning window sill.\",\n",
      "  \"intent_extraction\": {\n",
      "    \"modality\": \"photo\",\n",
      "    \"domain\": \"natural\",\n",
      "    \"primary_subject\": \"cup of coffee\",\n",
      "    \"context_scene\": \"rainy morning window sill\",\n",
      "    \"style_adjectives\": []\n",
      "  },\n",
      "  \"realism_scoring\": {\n",
      "    \"realism_score\": 0.9,\n",
      "    \"abstractness_score\": 0.1,\n",
      "    \"rationale\": \"The subject of a cup of coffee is highly plausible in the context of a rainy morning on a window sill, as it evokes a common and relatable scene. There are no modifiers present to introduce abstraction, reinforcing the realism of the scenario. Overall, this concept is very realistic and lacks imaginative elements.\"\n",
      "  },\n",
      "  \"feasibility_judgement\": {\n",
      "    \"feasibility_label\": \"feasible\",\n",
      "    \"realism_overall_score\": 0.9,\n",
      "    \"creative_potential_score\": 0.1,\n",
      "    \"summary\": \"The scenario of a cup of coffee on a rainy morning window sill is highly plausible and relatable, making it feasible for image generation. The high realism score reflects its grounded nature, while the low creative potential indicates a lack of imaginative elements.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "'Semantic_map.json' saved for next pipeline steps.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# PROMPT UNDERSTANDING MODULE (Step 1 of Media Fetching Framework)\n",
    "# ===============================================================\n",
    "\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# üß† STEP 1: Intent Extraction\n",
    "# =========================\n",
    "\n",
    "user_prompt = input(\"Enter your media generation prompt: \")\n",
    "\n",
    "intent_prompt = f\"\"\"\n",
    "You are an AI system analyzing a creative prompt for intent and structure.\n",
    "\n",
    "Prompt: \"{user_prompt}\"\n",
    "\n",
    "Perform Intent Extraction with these substeps:\n",
    "1. **Modality** ‚Äî photo, video, animation, artwork, etc.\n",
    "2. **Domain** ‚Äî natural, artistic, conceptual, surreal, etc.\n",
    "3. **Primary Subject** ‚Äî the main focus entity.\n",
    "4. **Context / Scene** ‚Äî environment or setting.\n",
    "5. **Style Adjectives** ‚Äî mood or tone modifiers (e.g. cinematic, slow-motion, dreamy).\n",
    "\n",
    "Return a JSON object exactly in this format:\n",
    "{{\n",
    "  \"modality\": \"\",\n",
    "  \"domain\": \"\",\n",
    "  \"primary_subject\": \"\",\n",
    "  \"context_scene\": \"\",\n",
    "  \"style_adjectives\": []\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "intent_json = ask_llm(intent_prompt)\n",
    "print(\"üîç Intent Extraction Result:\\n\", intent_json)\n",
    "\n",
    "try:\n",
    "    intent_data = json.loads(intent_json)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Error decoding JSON ‚Äî please check model output.\")\n",
    "    intent_data = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# üé® STEP 2: Realism & Abstractness Scoring\n",
    "# =========================\n",
    "\n",
    "score_prompt = f\"\"\"\n",
    "You are a cognitive model that evaluates the realism and abstractness of a visual prompt.\n",
    "\n",
    "Use the following extracted intent:\n",
    "{json.dumps(intent_data, indent=2)}\n",
    "\n",
    "Perform analysis based on these three information sources:\n",
    "1. Subject‚ÄìContext Relationship: how plausible is the subject existing in this context?\n",
    "2. Modifiers: do adjectives or style words (like 'glowing', 'ethereal', 'cinematic') add creative abstraction?\n",
    "3. Domain Knowledge: using world understanding, judge how realistic this concept would be.\n",
    "\n",
    "From this reasoning, generate TWO quantitative scores between 0.0 and 1.0:\n",
    "- realism_score: higher means physically and contextually plausible.\n",
    "- abstractness_score: higher means more imaginative, conceptual, or surreal.\n",
    "\n",
    "Also provide a short rationale describing your reasoning.\n",
    "\n",
    "Return ONLY valid JSON in the format:\n",
    "{{\n",
    "  \"realism_score\": 0.0,\n",
    "  \"abstractness_score\": 0.0,\n",
    "  \"rationale\": \"concise explanation summarizing how subject-context, modifiers, and domain knowledge influenced the scores.\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "scores_json = ask_llm(score_prompt)\n",
    "print(\"\\nüéØ Realism & Abstractness Scoring:\\n\", scores_json)\n",
    "\n",
    "try:\n",
    "    score_data = json.loads(scores_json)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Could not parse scores JSON.\")\n",
    "    score_data = {}\n",
    "\n",
    "# =========================\n",
    "# üåç STEP 3: Feasibility Judgement\n",
    "# =========================\n",
    "\n",
    "feasibility_prompt = f\"\"\"\n",
    "Now, using your world knowledge, make an overall feasibility judgement.\n",
    "\n",
    "Given:\n",
    "Intent: {json.dumps(intent_data, indent=2)}\n",
    "Scores: {json.dumps(score_data, indent=2)}\n",
    "\n",
    "Evaluate:\n",
    "- Is this scenario physically plausible?\n",
    "- Would an image/video generator find real reference material for this?\n",
    "- How realistic vs. creative is this concept overall?\n",
    "\n",
    "Return a JSON with:\n",
    "{{\n",
    "  \"feasibility_label\": \"feasible / partially_feasible / fantasy\",\n",
    "  \"realism_overall_score\": 0.0,\n",
    "  \"creative_potential_score\": 0.0,\n",
    "  \"summary\": \"short explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "feasibility_json = ask_llm(feasibility_prompt)\n",
    "print(\"\\nüåê Feasibility Judgement:\\n\", feasibility_json)\n",
    "\n",
    "try:\n",
    "    feasibility_data = json.loads(feasibility_json)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Could not parse feasibility JSON.\")\n",
    "    feasibility_data = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# üß© STEP 4: Semantic Map Assembly\n",
    "# =========================\n",
    "\n",
    "semantic_map = {\n",
    "    \"user_prompt\": user_prompt,\n",
    "    \"intent_extraction\": intent_data,\n",
    "    \"realism_scoring\": score_data,\n",
    "    \"feasibility_judgement\": feasibility_data\n",
    "}\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Semantic Map:\\n\")\n",
    "print(json.dumps(semantic_map, indent=2))\n",
    "\n",
    "# Save for later use\n",
    "with open(\"semantic_map.json\", \"w\") as f:\n",
    "    json.dump(semantic_map, f, indent=2)\n",
    "\n",
    "# =========================\n",
    "# üìä STEP 5: Visualize Scores\n",
    "# =========================\n",
    "\n",
    "# Combine numeric scores\n",
    "numeric_scores = {\n",
    "    \"Subject+Context\": score_data.get(\"subject_context_score\", 0),\n",
    "    \"Modifier Abstractness\": score_data.get(\"modifier_abstractness_score\", 0),\n",
    "    \"Domain Feasibility\": score_data.get(\"domain_feasibility_score\", 0),\n",
    "    \"Overall Realism\": feasibility_data.get(\"realism_overall_score\", 0),\n",
    "    \"Creative Potential\": feasibility_data.get(\"creative_potential_score\", 0),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(list(numeric_scores.items()), columns=[\"Dimension\", \"Score\"])\n",
    "\n",
    "print(\"\\n'Semantic_map.json' saved for next pipeline steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b48b46",
   "metadata": {},
   "source": [
    "A cat sleeping on a sunny window sill with gentle morning light.\n",
    "\n",
    "A glowing jellyfish flying over a city at night in cinematic slow motion.\n",
    "\n",
    "A futuristic train made of glass traveling across floating islands.\n",
    "\n",
    "An astronaut planting flowers on the surface of Mars.\n",
    "\n",
    "A black-and-white photograph of an old man reading under a streetlamp.\n",
    "\n",
    "A dragon curled up inside a modern-day subway station.\n",
    "\n",
    "A realistic portrait of a woman made entirely of blooming roses.\n",
    "\n",
    "A time-lapse video of clouds forming the shape of running horses.\n",
    "\n",
    "A candle melting backward, reconstructing itself in reverse motion.\n",
    "\n",
    "A neon-lit cyberpunk street with rain reflections and flying cars.\n",
    "\n",
    "A microscopic view of snowflakes forming musical notes.\n",
    "\n",
    "A realistic dog jumping into a swimming pool during summer.\n",
    "\n",
    "A painting of an ocean made of glass with sunlight refracting inside.\n",
    "\n",
    "A video showing trees breathing like living creatures at dusk.\n",
    "\n",
    "A simple photo of a cup of coffee on a rainy morning window sill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Decision Reasoning Result:\n",
      " {\n",
      "  \"reasoning_trace\": \"The scenario described is highly realistic and relatable, making it feasible to retrieve a real photo of a cup of coffee on a rainy morning window sill. The high realism score (0.9) and low abstractness score (0.1) suggest that this scene is common and easily found in existing media. Given the feasibility label is 'feasible' and the creative potential score is low, it indicates that generating a synthetic image may not add significant value. Therefore, fetching a real image is the most logical choice.\",\n",
      "  \"cost_latency_estimate\": {\n",
      "      \"fetch\": {\"cost\": \"low\", \"latency\": \"medium\"},\n",
      "      \"generate\": {\"cost\": \"medium\", \"latency\": \"high\"}\n",
      "  },\n",
      "  \"final_decision\": \"fetch_from_database_or_web\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "üß† Decision Reasoning Summary:\n",
      "{\n",
      "  \"reasoning_trace\": \"The scenario described is highly realistic and relatable, making it feasible to retrieve a real photo of a cup of coffee on a rainy morning window sill. The high realism score (0.9) and low abstractness score (0.1) suggest that this scene is common and easily found in existing media. Given the feasibility label is 'feasible' and the creative potential score is low, it indicates that generating a synthetic image may not add significant value. Therefore, fetching a real image is the most logical choice.\",\n",
      "  \"cost_latency_estimate\": {\n",
      "    \"fetch\": {\n",
      "      \"cost\": \"low\",\n",
      "      \"latency\": \"medium\"\n",
      "    },\n",
      "    \"generate\": {\n",
      "      \"cost\": \"medium\",\n",
      "      \"latency\": \"high\"\n",
      "    }\n",
      "  },\n",
      "  \"final_decision\": \"fetch_from_database_or_web\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "‚úÖ Updated semantic_map.json saved with decision reasoning results.\n",
      "\n",
      "üß© Decision Reasoning Result:\n",
      " {\n",
      "  \"reasoning_trace\": \"The scenario of a cup of coffee on a rainy morning window sill is highly realistic and relatable, with a realism score of 0.9 and an abstractness score of 0.1. This indicates that such images are common and easily found in existing media. The feasibility label is 'feasible' and the creative potential score is low, suggesting that generating a synthetic image may not provide additional value. Therefore, fetching a real image is the most logical choice.\",\n",
      "  \"cost_latency_estimate\": {\n",
      "      \"fetch\": {\"cost\": \"low\", \"latency\": \"medium\"},\n",
      "      \"generate\": {\"cost\": \"medium\", \"latency\": \"high\"}\n",
      "  },\n",
      "  \"final_decision\": \"fetch_from_web\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "üß† Decision Reasoning Summary:\n",
      "{\n",
      "  \"reasoning_trace\": \"The scenario of a cup of coffee on a rainy morning window sill is highly realistic and relatable, with a realism score of 0.9 and an abstractness score of 0.1. This indicates that such images are common and easily found in existing media. The feasibility label is 'feasible' and the creative potential score is low, suggesting that generating a synthetic image may not provide additional value. Therefore, fetching a real image is the most logical choice.\",\n",
      "  \"cost_latency_estimate\": {\n",
      "    \"fetch\": {\n",
      "      \"cost\": \"low\",\n",
      "      \"latency\": \"medium\"\n",
      "    },\n",
      "    \"generate\": {\n",
      "      \"cost\": \"medium\",\n",
      "      \"latency\": \"high\"\n",
      "    }\n",
      "  },\n",
      "  \"final_decision\": \"fetch_from_web\",\n",
      "  \"confidence\": 0.95\n",
      "}\n",
      "\n",
      "‚úÖ Updated semantic_map.json saved with decision reasoning results.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# DECISION REASONING MODULE (Step 2 of Media Fetching Framework)\n",
    "# ===============================================================\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load your semantic map\n",
    "with open(\"semantic_map.json\", \"r\") as f:\n",
    "    semantic_map = json.load(f)\n",
    "\n",
    "# ===============================================================\n",
    "# DECISION REASONING MODULE (Step 2 of Media Fetching Framework)\n",
    "# ===============================================================\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load your semantic map\n",
    "with open(\"semantic_map.json\", \"r\") as f:\n",
    "    semantic_map = json.load(f)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))    \n",
    "def ask_llm(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI decision reasoning module for a media-fetching system.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# =========================\n",
    "# üß† STEP 1: Contextual Chain-of-Thought Reasoning\n",
    "# =========================\n",
    "\n",
    "decision_prompt = f\"\"\"\n",
    "You are performing *Decision Reasoning* based on the following semantic map:\n",
    "\n",
    "{json.dumps(semantic_map, indent=2)}\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. **Perform contextual reasoning**:\n",
    "   - Examine the realism_score and abstractness_score.\n",
    "   - Consider the feasibility_label and creative_potential_score.\n",
    "   - Think step-by-step about what kind of media retrieval makes sense.\n",
    "\n",
    "2. **Estimate Cost and Latency**:\n",
    "   Estimate the computational cost and time latency for both options:\n",
    "   - Fetch (search or retrieve real media)\n",
    "   - Generate (create synthetic image/video)\n",
    "   Classify each as: \"low\", \"medium\", or \"high\" cost and latency.\n",
    "\n",
    "3. **Decide Fetch vs Generate**:\n",
    "   Based on your reasoning, choose ONE of:\n",
    "   - \"fetch_from_database_or_web\"\n",
    "   - \"generate_with_model\"\n",
    "   - \"hybrid_fetch_and_enhance\"\n",
    "\n",
    "4. Return the result as JSON in this format:\n",
    "{{\n",
    "  \"reasoning_trace\": \"Your short reasoning chain-of-thought (natural language, concise).\",\n",
    "  \"cost_latency_estimate\": {{\n",
    "      \"fetch\": {{\"cost\": \"\", \"latency\": \"\"}},\n",
    "      \"generate\": {{\"cost\": \"\", \"latency\": \"\"}}\n",
    "  }},\n",
    "  \"final_decision\": \"\",\n",
    "  \"confidence\": 0.0\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "decision_json = ask_llm(decision_prompt)\n",
    "print(\"\\nüß© Decision Reasoning Result:\\n\", decision_json)\n",
    "\n",
    "try:\n",
    "    decision_data = json.loads(decision_json)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Could not parse decision JSON.\")\n",
    "    decision_data = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# üìä STEP 2: Display Decision Summary\n",
    "# =========================\n",
    "\n",
    "print(\"\\nüß† Decision Reasoning Summary:\")\n",
    "print(json.dumps(decision_data, indent=2))\n",
    "\n",
    "# Merge back into master pipeline\n",
    "semantic_map[\"decision_reasoning\"] = decision_data\n",
    "\n",
    "# Save updated map\n",
    "with open(\"semantic_map.json\", \"w\") as f:\n",
    "    json.dump(semantic_map, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Updated semantic_map.json saved with decision reasoning results.\")\n",
    "\n",
    "# =========================\n",
    "# üß† STEP 1: Contextual Chain-of-Thought Reasoning\n",
    "# =========================\n",
    "\n",
    "decision_prompt = f\"\"\"\n",
    "You are performing *Decision Reasoning* based on the following semantic map:\n",
    "\n",
    "{json.dumps(semantic_map, indent=2)}\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. **Perform contextual reasoning**:\n",
    "   - Examine the realism_score and abstractness_score.\n",
    "   - Consider the feasibility_label and creative_potential_score.\n",
    "   - Think step-by-step about what kind of media retrieval makes sense.\n",
    "\n",
    "2. **Estimate Cost and Latency**:\n",
    "   Estimate the computational cost and time latency for both options:\n",
    "   - Fetch (search or retrieve real media)\n",
    "   - Generate (create synthetic image/video)\n",
    "   Classify each as: \"low\", \"medium\", or \"high\" cost and latency.\n",
    "\n",
    "3. **Decide Fetch vs Generate**:\n",
    "   Based on your reasoning, choose ONE of:\n",
    "   - \"fetch_from_web\"\n",
    "   - \"generate_with_model\"\n",
    "   - \"hybrid_fetch_and_enhance\"\n",
    "\n",
    "4. Return the result as JSON in this format:\n",
    "{{\n",
    "  \"reasoning_trace\": \"Your short reasoning chain-of-thought (natural language, concise).\",\n",
    "  \"cost_latency_estimate\": {{\n",
    "      \"fetch\": {{\"cost\": \"\", \"latency\": \"\"}},\n",
    "      \"generate\": {{\"cost\": \"\", \"latency\": \"\"}}\n",
    "  }},\n",
    "  \"final_decision\": \"\",\n",
    "  \"confidence\": 0.0\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "decision_json = ask_llm(decision_prompt)\n",
    "print(\"\\nüß© Decision Reasoning Result:\\n\", decision_json)\n",
    "\n",
    "try:\n",
    "    decision_data = json.loads(decision_json)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Could not parse decision JSON.\")\n",
    "    decision_data = {}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# üìä STEP 2: Display Decision Summary\n",
    "# =========================\n",
    "\n",
    "print(\"\\nüß† Decision Reasoning Summary:\")\n",
    "print(json.dumps(decision_data, indent=2))\n",
    "\n",
    "# Merge back into master pipeline\n",
    "semantic_map[\"decision_reasoning\"] = decision_data\n",
    "\n",
    "# Save updated map\n",
    "with open(\"semantic_map.json\", \"w\") as f:\n",
    "    json.dump(semantic_map, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Updated semantic_map.json saved with decision reasoning results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_unsplash_and_pexels_selenium.py\n",
    "# requirements:\n",
    "# pip install selenium webdriver-manager requests matplotlib pillow\n",
    "# (This script uses webdriver-manager to auto-download chromedriver)\n",
    "#\n",
    "# Note: scraping websites may be restricted by their terms/robots.txt.\n",
    "# You said the previous Selenium approach worked for you ‚Äî this script follows\n",
    "# the same pattern and scrapes both Unsplash and Pexels search pages.\n",
    "# Use responsibly.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEST_DIR = \"scrape_assets\"\n",
    "IMAGES_DIR = os.path.join(DEST_DIR, \"images\")\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "METADATA_PATH = os.path.join(DEST_DIR, \"metadata.json\")\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "MIN_PIXELS = 120 * 120  # minimum pixel area to keep an image\n",
    "HEADLESS = True\n",
    "\n",
    "# -----------------------------\n",
    "# WebDriver\n",
    "# -----------------------------\n",
    "def create_driver(headless=HEADLESS):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f\"user-agent={USER_AGENT}\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "# -----------------------------\n",
    "# Utility helpers\n",
    "# -----------------------------\n",
    "def choose_best_image_src(src, srcset):\n",
    "    if srcset:\n",
    "        try:\n",
    "            parts = [p.strip() for p in srcset.split(\",\") if p.strip()]\n",
    "            last = parts[-1]\n",
    "            url = last.split(\" \")[0]\n",
    "            return url\n",
    "        except Exception:\n",
    "            pass\n",
    "    return src\n",
    "\n",
    "def sha256_bytes(b):\n",
    "    import hashlib\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def download_bytes(url, timeout=20):\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    r = requests.get(url, headers=headers, timeout=timeout, stream=True)\n",
    "    r.raise_for_status()\n",
    "    content = r.content\n",
    "    return content\n",
    "\n",
    "def inspect_image_bytes(content):\n",
    "    try:\n",
    "        im = Image.open(BytesIO(content))\n",
    "        w, h = im.size\n",
    "        return w, h\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "# -----------------------------\n",
    "# Site-specific scrapers (Selenium + heuristics)\n",
    "# -----------------------------\n",
    "def scrape_unsplash_search(driver, query, max_images=20, scroll_pause=1.2):\n",
    "    \"\"\"Return list of candidate dicts: {img_url, alt, photographer, page_url, source}\"\"\"\n",
    "    url = f\"https://unsplash.com/s/photos/{query.replace(' ', '-')}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    # scroll to lazy load\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    imgs_found = []\n",
    "    seen = set()\n",
    "    while len(imgs_found) < max_images:\n",
    "        imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in imgs:\n",
    "            try:\n",
    "                src = img.get_attribute(\"src\") or \"\"\n",
    "                srcset = img.get_attribute(\"srcset\") or \"\"\n",
    "                alt = img.get_attribute(\"alt\") or \"\"\n",
    "                best = choose_best_image_src(src, srcset)\n",
    "                if not best:\n",
    "                    continue\n",
    "                # prefer unsplash-hosted images\n",
    "                if \"images.unsplash.com\" not in best and \"/photos/\" not in best and \"photo-\" not in best:\n",
    "                    continue\n",
    "                dedupe = best.split(\"?\")[0]\n",
    "                if dedupe in seen:\n",
    "                    continue\n",
    "                # attempt to find photographer/page link\n",
    "                photographer = \"\"\n",
    "                page_url = \"\"\n",
    "                try:\n",
    "                    parent_a = img.find_element(By.XPATH, \"./ancestor::a[contains(@href,'unsplash.com/photos')][1]\")\n",
    "                    page_url = parent_a.get_attribute(\"href\") or \"\"\n",
    "                    # photographer often available in parent figure or siblings; fallback to alt\n",
    "                except Exception:\n",
    "                    # fallback: find nearest link\n",
    "                    try:\n",
    "                        any_a = img.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "                        page_url = any_a.get_attribute(\"href\") or \"\"\n",
    "                    except Exception:\n",
    "                        page_url = \"\"\n",
    "                # Add candidate\n",
    "                seen.add(dedupe)\n",
    "                imgs_found.append({\n",
    "                    \"img_url\": best,\n",
    "                    \"alt\": alt,\n",
    "                    \"photographer\": photographer,\n",
    "                    \"page_url\": page_url,\n",
    "                    \"source\": \"unsplash\"\n",
    "                })\n",
    "                if len(imgs_found) >= max_images:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            # small wait and break if still same\n",
    "            time.sleep(scroll_pause * 1.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "    return imgs_found[:max_images]\n",
    "\n",
    "def scrape_pexels_search(driver, query, max_images=20, scroll_pause=1.2):\n",
    "    \"\"\"Return list of candidate dicts for Pexels search pages.\"\"\"\n",
    "    url = f\"https://www.pexels.com/search/{query.replace(' ', '%20')}/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    imgs_found = []\n",
    "    seen = set()\n",
    "    while len(imgs_found) < max_images:\n",
    "        # Pexels uses many <img> tags; prefer images with 'src' or 'data-src'\n",
    "        imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in imgs:\n",
    "            try:\n",
    "                src = img.get_attribute(\"src\") or img.get_attribute(\"data-src\") or \"\"\n",
    "                srcset = img.get_attribute(\"srcset\") or \"\"\n",
    "                alt = img.get_attribute(\"alt\") or \"\"\n",
    "                best = choose_best_image_src(src, srcset) or src\n",
    "                if not best:\n",
    "                    continue\n",
    "                # skip tiny UI avatars or badges\n",
    "                if \"images.pexels.com\" not in best and \"pexels\" not in best:\n",
    "                    continue\n",
    "                dedupe = best.split(\"?\")[0]\n",
    "                if dedupe in seen:\n",
    "                    continue\n",
    "                # find parent link (pin/photo page)\n",
    "                page_url = \"\"\n",
    "                try:\n",
    "                    parent_a = img.find_element(By.XPATH, \"./ancestor::a[contains(@href,'/photo/')][1]\")\n",
    "                    page_url = parent_a.get_attribute(\"href\") or \"\"\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        any_a = img.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "                        page_url = any_a.get_attribute(\"href\") or \"\"\n",
    "                    except Exception:\n",
    "                        page_url = \"\"\n",
    "                seen.add(dedupe)\n",
    "                imgs_found.append({\n",
    "                    \"img_url\": best,\n",
    "                    \"alt\": alt,\n",
    "                    \"photographer\": \"\",\n",
    "                    \"page_url\": page_url,\n",
    "                    \"source\": \"pexels\"\n",
    "                })\n",
    "                if len(imgs_found) >= max_images:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            time.sleep(scroll_pause * 1.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "    return imgs_found[:max_images]\n",
    "\n",
    "# -----------------------------\n",
    "# Download candidates, dedupe, quality-check, save metadata\n",
    "# -----------------------------\n",
    "def run_scrape(query, max_per_site=24, driver_headless=True):\n",
    "    driver = create_driver(headless=driver_headless)\n",
    "    try:\n",
    "        all_candidates = []\n",
    "        print(f\"[seed] unsplash -> searching for: {query}\")\n",
    "        try:\n",
    "            unsplash_cands = scrape_unsplash_search(driver, query, max_images=max_per_site)\n",
    "            print(f\"[scrape] found {len(unsplash_cands)} candidates on unsplash\")\n",
    "            all_candidates.extend(unsplash_cands)\n",
    "        except Exception as e:\n",
    "            print(\"[error] Unsplash scrape failed:\", e)\n",
    "\n",
    "        print(f\"[seed] pexels -> searching for: {query}\")\n",
    "        try:\n",
    "            pexels_cands = scrape_pexels_search(driver, query, max_images=max_per_site)\n",
    "            print(f\"[scrape] found {len(pexels_cands)} candidates on pexels\")\n",
    "            all_candidates.extend(pexels_cands)\n",
    "        except Exception as e:\n",
    "            print(\"[error] Pexels scrape failed:\", e)\n",
    "\n",
    "        print(f\"[pipeline] total raw candidates: {len(all_candidates)}\")\n",
    "\n",
    "        # download + dedupe + quality\n",
    "        metadata = []\n",
    "        seen_hashes = set()\n",
    "        idx = 0\n",
    "        for cand in all_candidates:\n",
    "            url = cand.get(\"img_url\")\n",
    "            if not url:\n",
    "                continue\n",
    "            if url.startswith(\"//\"):\n",
    "                url = \"https:\" + url\n",
    "            try:\n",
    "                # Try HEAD first to filter obviously tiny or non-image responses\n",
    "                try:\n",
    "                    head = requests.head(url, headers={\"User-Agent\": USER_AGENT}, timeout=8, allow_redirects=True)\n",
    "                    ct = head.headers.get(\"content-type\", \"\")\n",
    "                    cl = head.headers.get(\"content-length\")\n",
    "                    if ct and not ct.startswith(\"image\"):\n",
    "                        print(\"[skip] not an image (HEAD):\", url)\n",
    "                        continue\n",
    "                    if cl and int(cl) < 5_000:\n",
    "                        print(\"[skip] tiny content-length (HEAD):\", url)\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                content = download_bytes(url)\n",
    "                h = sha256_bytes(content)\n",
    "                if h in seen_hashes:\n",
    "                    print(\"[dedupe] duplicate:\", url)\n",
    "                    continue\n",
    "                w, h_px = inspect_image_bytes(content)\n",
    "                if w * h_px < MIN_PIXELS:\n",
    "                    print(f\"[quality] skipping small image {w}x{h_px} -> {url}\")\n",
    "                    continue\n",
    "\n",
    "                # save file\n",
    "                idx += 1\n",
    "                ext = \".jpg\"\n",
    "                # try detect format via PIL if needed\n",
    "                try:\n",
    "                    im = Image.open(BytesIO(content))\n",
    "                    fmt = im.format.lower() if im.format else \"jpg\"\n",
    "                    if fmt == \"png\":\n",
    "                        ext = \".png\"\n",
    "                    elif fmt == \"gif\":\n",
    "                        ext = \".gif\"\n",
    "                    elif fmt == \"webp\":\n",
    "                        ext = \".webp\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                filename = f\"{cand['source']}_{idx}{ext}\"\n",
    "                filepath = os.path.join(IMAGES_DIR, filename)\n",
    "                with open(filepath, \"wb\") as fh:\n",
    "                    fh.write(content)\n",
    "\n",
    "                seen_hashes.add(sha256_bytes(content))\n",
    "                meta = {\n",
    "                    \"id\": idx,\n",
    "                    \"filename\": filepath,\n",
    "                    \"source\": cand.get(\"source\"),\n",
    "                    \"img_url\": url,\n",
    "                    \"page_url\": cand.get(\"page_url\"),\n",
    "                    \"alt\": cand.get(\"alt\"),\n",
    "                    \"photographer\": cand.get(\"photographer\"),\n",
    "                    \"width\": w,\n",
    "                    \"height\": h_px,\n",
    "                    \"sha256\": sha256_bytes(content)\n",
    "                }\n",
    "                metadata.append(meta)\n",
    "                print(f\"[saved] {filepath} ({w}x{h_px})\")\n",
    "\n",
    "                # polite pause\n",
    "                time.sleep(0.25)\n",
    "            except Exception as e:\n",
    "                print(\"[download_error]\", e)\n",
    "                continue\n",
    "\n",
    "        # write metadata\n",
    "        with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(metadata, fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"[done] saved {len(metadata)} assets and metadata -> {METADATA_PATH}\")\n",
    "        return metadata\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: display a grid of thumbnails\n",
    "# -----------------------------\n",
    "def display_images_from_metadata(metadata, cols=4, thumb_size=(360, 240)):\n",
    "    files = [m[\"filename\"] for m in metadata if os.path.exists(m[\"filename\"])]\n",
    "    if not files:\n",
    "        print(\"[display] no images to show\")\n",
    "        return\n",
    "    rows = (len(files) + cols - 1) // cols\n",
    "    plt.figure(figsize=(cols * 3.5, rows * 2.8))\n",
    "    for i, f in enumerate(files, 1):\n",
    "        try:\n",
    "            im = Image.open(f).convert(\"RGB\")\n",
    "            thumb = Image.new(\"RGB\", thumb_size, (255,255,255))\n",
    "            # fit\n",
    "            im.thumbnail(thumb_size, Image.LANCZOS)\n",
    "            # paste centered\n",
    "            x = (thumb_size[0] - im.width) // 2\n",
    "            y = (thumb_size[1] - im.height) // 2\n",
    "            thumb.paste(im, (x, y))\n",
    "            plt.subplot(rows, cols, i)\n",
    "            plt.imshow(thumb)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(os.path.basename(f), fontsize=8)\n",
    "        except Exception:\n",
    "            continue\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter search query (e.g. 'girl dancing'): \").strip() or \"girl dancing\"\n",
    "    metadata = run_scrape(query, max_per_site=24, driver_headless=HEADLESS)\n",
    "    # display first results\n",
    "    try:\n",
    "        display_images_from_metadata(metadata, cols=4, thumb_size=(360,240))\n",
    "    except Exception as e:\n",
    "        print(\"Display failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85edd6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llm] generating photo-search queries from semantic_map...\n",
      "[llm] queries: ['cup of coffee rainy morning', 'coffee on window sill', 'rainy window coffee', 'morning coffee cozy', 'coffee cup indoors', 'rainy day coffee']\n",
      "[pipeline] scraping for query: 'cup of coffee rainy morning'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_1.jpg (832x832)\n",
      "[saved] scrape_assets\\images\\unsplash_2.jpg (2000x3000)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_3.jpg (2000x1333)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_4.jpg (2000x3000)\n",
      "[quality] too small, skipping\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_5.jpg (2000x3000)\n",
      "[saved] scrape_assets\\images\\pexels_6.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_7.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_8.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_9.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_10.jpg (1600x2844)\n",
      "[saved] scrape_assets\\images\\pexels_11.jpg (1600x1071)\n",
      "[saved] scrape_assets\\images\\pexels_12.jpg (1600x2397)\n",
      "[saved] scrape_assets\\images\\pexels_13.jpg (1600x2214)\n",
      "[saved] scrape_assets\\images\\pexels_14.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_15.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_16.jpg (1600x2000)\n",
      "[saved] scrape_assets\\images\\pexels_17.jpg (1600x2133)\n",
      "[pipeline] scraping for query: 'coffee on window sill'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[dedupe] duplicate image, skipping\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_18.jpg (2000x3000)\n",
      "[saved] scrape_assets\\images\\unsplash_19.jpg (2000x1333)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_20.jpg (2000x1333)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_21.jpg (2000x2312)\n",
      "[saved] scrape_assets\\images\\unsplash_22.jpg (2000x2667)\n",
      "[saved] scrape_assets\\images\\pexels_23.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_24.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_25.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_26.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_27.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_28.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_29.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_30.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_31.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_32.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_33.jpg (1600x1068)\n",
      "[saved] scrape_assets\\images\\pexels_34.jpg (1600x1068)\n",
      "[pipeline] scraping for query: 'rainy window coffee'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_35.jpg (832x832)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_36.jpg (2000x2000)\n",
      "[saved] scrape_assets\\images\\unsplash_37.jpg (2000x1325)\n",
      "[saved] scrape_assets\\images\\unsplash_38.jpg (2000x1325)\n",
      "[quality] too small, skipping\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_39.jpg (2000x3000)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\pexels_40.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_41.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_42.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_43.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_44.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_45.jpg (1600x2133)\n",
      "[saved] scrape_assets\\images\\pexels_46.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_47.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_48.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_49.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_50.jpg (1600x2133)\n",
      "[saved] scrape_assets\\images\\pexels_51.jpg (1600x981)\n",
      "[pipeline] scraping for query: 'morning coffee cozy'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_52.jpg (2000x3000)\n",
      "[saved] scrape_assets\\images\\unsplash_53.jpg (2000x2667)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_54.jpg (2000x3000)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_55.jpg (2000x1333)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_56.jpg (2000x2996)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_57.jpg (2000x3000)\n",
      "[saved] scrape_assets\\images\\pexels_58.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_59.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_60.jpg (1376x768)\n",
      "[dedupe] duplicate image, skipping\n",
      "[saved] scrape_assets\\images\\pexels_61.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_62.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_63.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_64.jpg (1600x2133)\n",
      "[saved] scrape_assets\\images\\pexels_65.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_66.jpg (1600x2842)\n",
      "[saved] scrape_assets\\images\\pexels_67.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_68.jpg (1600x2400)\n",
      "[pipeline] scraping for query: 'coffee cup indoors'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_69.jpg (832x832)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_70.jpg (2000x2998)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_71.jpg (2000x3020)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_72.jpg (2000x1509)\n",
      "[quality] too small, skipping\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_73.jpg (2000x3000)\n",
      "[saved] scrape_assets\\images\\pexels_74.jpg (1600x1884)\n",
      "[saved] scrape_assets\\images\\pexels_75.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_76.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_77.jpg (1600x2081)\n",
      "[saved] scrape_assets\\images\\pexels_78.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_79.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_80.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_81.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_82.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_83.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_84.jpg (1600x1067)\n",
      "[saved] scrape_assets\\images\\pexels_85.jpg (1600x1067)\n",
      "[pipeline] scraping for query: 'rainy day coffee'\n",
      "[pipeline] candidates returned: 24\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_86.jpg (832x832)\n",
      "[saved] scrape_assets\\images\\unsplash_87.jpg (2000x2449)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_88.jpg (2000x1600)\n",
      "[quality] too small, skipping\n",
      "[dedupe] duplicate image, skipping\n",
      "[saved] scrape_assets\\images\\unsplash_89.jpg (2000x3000)\n",
      "[quality] too small, skipping\n",
      "[saved] scrape_assets\\images\\pexels_90.jpg (1376x768)\n",
      "[dedupe] duplicate image, skipping\n",
      "[saved] scrape_assets\\images\\pexels_91.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_92.jpg (1376x768)\n",
      "[saved] scrape_assets\\images\\pexels_93.jpg (1600x2133)\n",
      "[saved] scrape_assets\\images\\pexels_94.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_95.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_96.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_97.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_98.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_99.jpg (1600x2400)\n",
      "[saved] scrape_assets\\images\\pexels_100.jpg (1600x2400)\n",
      "[done] scraped and saved 100 assets. metadata -> scrape_assets\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# fetch_from_semantic_map.py\n",
    "# Requirements:\n",
    "# pip install selenium webdriver-manager requests pillow openai python-dotenv\n",
    "# Set OPENAI_API_KEY in env before running\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# OpenAI client (new SDK)\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment. Query expansion via LLM will fail.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
    "\n",
    "# -----------------------------\n",
    "# Config (tweak as needed)\n",
    "# -----------------------------\n",
    "DEST_DIR = \"scrape_assets\"\n",
    "IMAGES_DIR = os.path.join(DEST_DIR, \"images\")\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "METADATA_PATH = os.path.join(DEST_DIR, \"metadata.json\")\n",
    "SEMANTIC_MAP_PATH = \"semantic_map.json\"\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "MIN_PIXELS = 120 * 120\n",
    "HEADLESS = True\n",
    "MAX_QUERIES = 6        # how many queries LLM should return (up to)\n",
    "MAX_IMAGES_PER_SITE = 12\n",
    "\n",
    "# -----------------------------\n",
    "# Reuse the working scraper functions (same pattern you had)\n",
    "# -----------------------------\n",
    "def create_driver(headless=HEADLESS):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(f\"user-agent={USER_AGENT}\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "def choose_best_image_src(src, srcset):\n",
    "    if srcset:\n",
    "        try:\n",
    "            parts = [p.strip() for p in srcset.split(\",\") if p.strip()]\n",
    "            return parts[-1].split(\" \")[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return src\n",
    "\n",
    "def download_bytes(url, timeout=20):\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    r = requests.get(url, headers=headers, timeout=timeout, stream=True)\n",
    "    r.raise_for_status()\n",
    "    return r.content, r.headers.get(\"content-type\", \"\")\n",
    "\n",
    "def inspect_image_bytes(content):\n",
    "    try:\n",
    "        im = Image.open(BytesIO(content))\n",
    "        return im.size  # (w, h)\n",
    "    except Exception:\n",
    "        return (0, 0)\n",
    "\n",
    "def sha256_bytes(b):\n",
    "    import hashlib\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "# Unsplash search scraping (Selenium)\n",
    "def scrape_unsplash_search(driver, query, max_images=MAX_IMAGES_PER_SITE, scroll_pause=1.2):\n",
    "    url = f\"https://unsplash.com/s/photos/{query.replace(' ', '-')}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    image_list = []\n",
    "    seen = set()\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while len(image_list) < max_images:\n",
    "        imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in imgs:\n",
    "            try:\n",
    "                src = img.get_attribute(\"src\") or \"\"\n",
    "                srcset = img.get_attribute(\"srcset\") or \"\"\n",
    "                alt = img.get_attribute(\"alt\") or \"\"\n",
    "                best = choose_best_image_src(src, srcset)\n",
    "                if not best:\n",
    "                    continue\n",
    "                # prefer Unsplash images\n",
    "                if \"images.unsplash.com\" not in best and \"/photos/\" not in best:\n",
    "                    continue\n",
    "                key = best.split(\"?\")[0]\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                # try to grab page url if possible\n",
    "                page_url = \"\"\n",
    "                try:\n",
    "                    parent = img.find_element(By.XPATH, \"./ancestor::a[contains(@href,'/photos')][1]\")\n",
    "                    page_url = parent.get_attribute(\"href\") or \"\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "                seen.add(key)\n",
    "                image_list.append({\n",
    "                    \"source\": \"unsplash\",\n",
    "                    \"img_url\": best,\n",
    "                    \"alt\": alt,\n",
    "                    \"page_url\": page_url\n",
    "                })\n",
    "                if len(image_list) >= max_images:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            time.sleep(scroll_pause * 1.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "    return image_list[:max_images]\n",
    "\n",
    "# Pexels search scraping (Selenium)\n",
    "def scrape_pexels_search(driver, query, max_images=MAX_IMAGES_PER_SITE, scroll_pause=1.2):\n",
    "    url = f\"https://www.pexels.com/search/{query.replace(' ', '%20')}/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    image_list = []\n",
    "    seen = set()\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while len(image_list) < max_images:\n",
    "        imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "        for img in imgs:\n",
    "            try:\n",
    "                src = img.get_attribute(\"src\") or img.get_attribute(\"data-src\") or \"\"\n",
    "                srcset = img.get_attribute(\"srcset\") or \"\"\n",
    "                alt = img.get_attribute(\"alt\") or \"\"\n",
    "                best = choose_best_image_src(src, srcset) or src\n",
    "                if not best:\n",
    "                    continue\n",
    "                # prefer Pexels-hosted images\n",
    "                if \"images.pexels.com\" not in best and \"pexels\" not in best:\n",
    "                    continue\n",
    "                key = best.split(\"?\")[0]\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                page_url = \"\"\n",
    "                try:\n",
    "                    parent = img.find_element(By.XPATH, \"./ancestor::a[contains(@href,'/photo/')][1]\")\n",
    "                    page_url = parent.get_attribute(\"href\") or \"\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "                seen.add(key)\n",
    "                image_list.append({\n",
    "                    \"source\": \"pexels\",\n",
    "                    \"img_url\": best,\n",
    "                    \"alt\": alt,\n",
    "                    \"page_url\": page_url\n",
    "                })\n",
    "                if len(image_list) >= max_images:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            time.sleep(scroll_pause * 1.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "    return image_list[:max_images]\n",
    "\n",
    "# Combined runner wrapper (calls both)\n",
    "def run_scrape_for_query(query, driver, max_per_site=MAX_IMAGES_PER_SITE):\n",
    "    results = []\n",
    "    try:\n",
    "        us = scrape_unsplash_search(driver, query, max_images=max_per_site)\n",
    "    except Exception as e:\n",
    "        print(\"[unsplash scrape error]\", e)\n",
    "        us = []\n",
    "    try:\n",
    "        px = scrape_pexels_search(driver, query, max_images=max_per_site)\n",
    "    except Exception as e:\n",
    "        print(\"[pexels scrape error]\", e)\n",
    "        px = []\n",
    "    results.extend(us)\n",
    "    results.extend(px)\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# LLM prompt for query generation\n",
    "# -----------------------------\n",
    "def build_query_generation_prompt(semantic_map, max_queries=MAX_QUERIES):\n",
    "    \"\"\"\n",
    "    Construct a robust prompt for the LLM to produce short search queries optimized for photo sites.\n",
    "    NOTE: literal braces in the example JSON are escaped as {{ and }} so f-string doesn't try to format them.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an assistant that converts a user's creative media intent into short, effective search phrases optimized\n",
    "for visual stock/photo sites such as Unsplash and Pexels.\n",
    "\n",
    "Input semantic_map (JSON):\n",
    "{json.dumps(semantic_map, indent=2)}\n",
    "\n",
    "Task:\n",
    "1. Read the semantic_map. Your goal is to produce up to {max_queries} concise search queries (each 2‚Äì6 words) that are likely to return **real photographic assets** on Unsplash/Pexels.\n",
    "2. Avoid returning the long user prompt verbatim. Instead, produce short keyword-rich phrases combining:\n",
    "   - main subject noun (people/animal/object)\n",
    "   - action or pose (if relevant)\n",
    "   - color / lighting / mood (optional)\n",
    "   - orientation or framing tokens like \"portrait\", \"landscape\", \"closeup\", \"aerial\" (optional)\n",
    "   - genre tags like \"street\", \"portrait\", \"landscape\", \"studio\"\n",
    "3. Do NOT include punctuation, quotes, or sentence fragments ‚Äî only short search phrases (lowercase preferred).\n",
    "4. Prioritize realistic photographic cues (e.g., \"girl dancing portrait golden hour\", \"clouds dramatic sunset\", \"dog jumping pool summer\") rather than abstract/surreal descriptors (if the semantic_map indicates high realism_score, prefer concrete queries; if abstractness is high, include queries that may match creative composites like \"surreal cloud shape\", but still short).\n",
    "5. Return EXACTLY ONE JSON object with key \"queries\" whose value is a list of unique phrases.\n",
    "\n",
    "Example valid output (note the braces are shown literally here):\n",
    "{{\"queries\": [\"girl dancing portrait golden hour\", \"woman dancing street candid\"]}}\n",
    "\n",
    "Now produce the JSON object for the provided semantic_map.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def ask_llm_for_queries(semantic_map, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Ask the LLM to generate short search queries. Returns a dict with key \"queries\".\n",
    "    Falls back to heuristic extraction if OpenAI client is not configured.\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        print(\"OpenAI client not available ‚Äî falling back to simple keyword extraction from semantic_map\")\n",
    "        intent = semantic_map.get(\"intent_extraction\", {})\n",
    "        subj = intent.get(\"primary_subject\", \"\").split()[:3]\n",
    "        ctx = intent.get(\"context_scene\", \"\").split()[:3]\n",
    "        styles = intent.get(\"style_adjectives\", [])[:2] if isinstance(intent.get(\"style_adjectives\"), list) else []\n",
    "        q1 = \" \".join([t for t in subj + ctx + styles if t]).strip()\n",
    "        q2 = \" \".join([t for t in subj + [\"portrait\"] if t]).strip()\n",
    "        qs = [q for q in [q1, q2] if q]\n",
    "        return {\"queries\": qs[:MAX_QUERIES]}\n",
    "\n",
    "    prompt = build_query_generation_prompt(semantic_map, max_queries=MAX_QUERIES)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You convert semantic intent into short search phrases for photo sites.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        txt = resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"[LLM] request failed:\", e)\n",
    "        return {\"queries\": []}\n",
    "\n",
    "    # Try to parse JSON directly, otherwise try to extract the first {...} block\n",
    "    try:\n",
    "        parsed = json.loads(txt)\n",
    "        if isinstance(parsed, dict) and \"queries\" in parsed:\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: extract first JSON object from the text\n",
    "    import re\n",
    "    m = re.search(r\"(\\{(?:[^{}]|(?R))*\\})\", txt, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        # simpler fallback: find a simple {\"queries\":[...]} pattern\n",
    "        m = re.search(r\"(\\{.*\\\"queries\\\"\\s*:\\s*\\[.*?\\].*\\})\", txt, flags=re.DOTALL)\n",
    "\n",
    "    if m:\n",
    "        try:\n",
    "            parsed = json.loads(m.group(1))\n",
    "            if isinstance(parsed, dict) and \"queries\" in parsed:\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Last resort: try to read lines and extract quoted phrases or lines\n",
    "    candidates = []\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip().strip('-‚Ä¢ ')\n",
    "        # if line contains a quoted phrase\n",
    "        qmatch = re.findall(r'\"([^\"]{2,80})\"|\\'([^\\']{2,80})\\'', line)\n",
    "        if qmatch:\n",
    "            for a, b in qmatch:\n",
    "                candidates.append((a or b).strip())\n",
    "            continue\n",
    "        # if line is short (2-6 words), accept it\n",
    "        if 1 < len(line.split()) <= 6:\n",
    "            # remove punctuation\n",
    "            cleaned = re.sub(r'[^\\w\\s-]', '', line).strip()\n",
    "            if cleaned:\n",
    "                candidates.append(cleaned)\n",
    "\n",
    "    # dedupe and trim\n",
    "    uniq = []\n",
    "    for c in candidates:\n",
    "        cl = c.lower()\n",
    "        if cl not in [u.lower() for u in uniq]:\n",
    "            uniq.append(c)\n",
    "    if uniq:\n",
    "        return {\"queries\": uniq[:MAX_QUERIES]}\n",
    "\n",
    "    # If nothing parsed, return empty\n",
    "    print(\"[LLM] Could not parse queries from response. Raw response:\")\n",
    "    print(txt)\n",
    "    return {\"queries\": []}\n",
    "\n",
    "# -----------------------------\n",
    "# Main orchestrator\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # load semantic map created earlier\n",
    "    if not os.path.exists(SEMANTIC_MAP_PATH):\n",
    "        raise FileNotFoundError(f\"{SEMANTIC_MAP_PATH} not found. Run prompt-understanding/decision stages first.\")\n",
    "    with open(SEMANTIC_MAP_PATH, \"r\", encoding=\"utf-8\") as fh:\n",
    "        semantic_map = json.load(fh)\n",
    "\n",
    "    # ask LLM to create concise queries\n",
    "    print(\"[llm] generating photo-search queries from semantic_map...\")\n",
    "    qobj = ask_llm_for_queries(semantic_map)\n",
    "    queries = qobj.get(\"queries\", [])[:MAX_QUERIES]\n",
    "    queries = [q.strip() for q in queries if q and len(q.strip()) > 1]\n",
    "    if not queries:\n",
    "        print(\"No queries produced by LLM ‚Äî aborting.\")\n",
    "        return\n",
    "\n",
    "    print(\"[llm] queries:\", queries)\n",
    "\n",
    "    # run scraping for each query\n",
    "    driver = create_driver(headless=HEADLESS)\n",
    "    try:\n",
    "        all_metadata = []\n",
    "        seen_hashes = set()\n",
    "        idx = 0\n",
    "        for q in queries:\n",
    "            print(f\"[pipeline] scraping for query: '{q}'\")\n",
    "            candidates = run_scrape_for_query(q, driver, max_per_site=MAX_IMAGES_PER_SITE)\n",
    "            print(f\"[pipeline] candidates returned: {len(candidates)}\")\n",
    "            for cand in candidates:\n",
    "                url = cand.get(\"img_url\")\n",
    "                if not url:\n",
    "                    continue\n",
    "                if url.startswith(\"//\"):\n",
    "                    url = \"https:\" + url\n",
    "                # quick HEAD filter\n",
    "                try:\n",
    "                    head = requests.head(url, headers={\"User-Agent\": USER_AGENT}, timeout=8, allow_redirects=True)\n",
    "                    ctype = head.headers.get(\"content-type\",\"\")\n",
    "                    cl = head.headers.get(\"content-length\")\n",
    "                    if ctype and not ctype.startswith(\"image\"):\n",
    "                        continue\n",
    "                    if cl and int(cl) < 3000:\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # download\n",
    "                try:\n",
    "                    content, ctype = download_bytes(url)\n",
    "                except Exception as e:\n",
    "                    print(\"[download failed]\", e)\n",
    "                    continue\n",
    "                digest = sha256_bytes(content)\n",
    "                if digest in seen_hashes:\n",
    "                    print(\"[dedupe] duplicate image, skipping\")\n",
    "                    continue\n",
    "                w, h = inspect_image_bytes(content)\n",
    "                if w * h < MIN_PIXELS:\n",
    "                    print(\"[quality] too small, skipping\")\n",
    "                    continue\n",
    "                idx += 1\n",
    "                # choose extension from content type or PIL\n",
    "                ext = \".jpg\"\n",
    "                if \"png\" in ctype:\n",
    "                    ext = \".png\"\n",
    "                elif \"gif\" in ctype:\n",
    "                    ext = \".gif\"\n",
    "                else:\n",
    "                    # try PIL format fallback\n",
    "                    try:\n",
    "                        im = Image.open(BytesIO(content))\n",
    "                        fmt = (im.format or \"\").lower()\n",
    "                        if fmt == \"png\":\n",
    "                            ext = \".png\"\n",
    "                        elif fmt == \"webp\":\n",
    "                            ext = \".webp\"\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                fname = f\"{cand.get('source','img')}_{idx}{ext}\"\n",
    "                fpath = os.path.join(IMAGES_DIR, fname)\n",
    "                with open(fpath, \"wb\") as fh:\n",
    "                    fh.write(content)\n",
    "                seen_hashes.add(digest)\n",
    "                meta_item = {\n",
    "                    \"id\": idx,\n",
    "                    \"filename\": fpath,\n",
    "                    \"source\": cand.get(\"source\"),\n",
    "                    \"img_url\": url,\n",
    "                    \"page_url\": cand.get(\"page_url\"),\n",
    "                    \"alt\": cand.get(\"alt\"),\n",
    "                    \"query_used\": q,\n",
    "                    \"width\": w,\n",
    "                    \"height\": h,\n",
    "                    \"sha256\": digest\n",
    "                }\n",
    "                all_metadata.append(meta_item)\n",
    "                print(f\"[saved] {fpath} ({w}x{h})\")\n",
    "                time.sleep(0.2)\n",
    "        # write metadata file and append to semantic_map\n",
    "        with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(all_metadata, fh, indent=2, ensure_ascii=False)\n",
    "        semantic_map[\"fetched_assets\"] = all_metadata\n",
    "        # Save updated semantic map\n",
    "        with open(SEMANTIC_MAP_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(semantic_map, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[done] scraped and saved {len(all_metadata)} assets. metadata -> {METADATA_PATH}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
